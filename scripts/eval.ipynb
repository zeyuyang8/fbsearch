{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "224b20be",
      "metadata": {},
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d50d63",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "def row2text_template_scifact(row):\n",
        "    return f\"Title: {row['title']}\\nAbstract: {' '.join(row['abstract'])}\\nStructured: {row['structured']}\\n\"\n",
        "\n",
        "\n",
        "class SciFactDataset(Dataset):\n",
        "    def __init__(self, queries_path, corpus_path):\n",
        "        super().__init__()\n",
        "        queries = pd.read_json(queries_path, lines=True)\n",
        "        corpus = pd.read_json(corpus_path, lines=True)\n",
        "        corpus[\"text\"] = corpus.apply(row2text_template_scifact, axis=1)\n",
        "\n",
        "        self.queries = queries\n",
        "        self.corpus = corpus\n",
        "        self.corpus_dict = corpus.set_index(\"doc_id\")[\"text\"].to_dict()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        doc_id_list = self.queries[\"cited_doc_ids\"][i]\n",
        "        query = self.queries[\"claim\"][i]\n",
        "\n",
        "        docs = [\n",
        "            self.corpus_dict.get(doc_id)\n",
        "            for doc_id in doc_id_list\n",
        "            if doc_id in self.corpus_dict\n",
        "        ]\n",
        "        n_docs = len(docs)\n",
        "\n",
        "        if n_docs == 0:\n",
        "            return {}\n",
        "        else:\n",
        "            queries = [query] * n_docs\n",
        "\n",
        "        return {\n",
        "            \"doc_id\": doc_id_list,\n",
        "            \"query\": queries,\n",
        "            \"text\": docs,\n",
        "        }\n",
        "\n",
        "\n",
        "def scifact_collate_fn(batch):\n",
        "    batch = [item for item in batch if item]\n",
        "    if not batch:\n",
        "        return {}\n",
        "    doc_ids = sum([item[\"doc_id\"] for item in batch], [])\n",
        "    queries = sum([item[\"query\"] for item in batch], [])\n",
        "    texts = sum([item[\"text\"] for item in batch], [])\n",
        "    return {\n",
        "        \"doc_id\": doc_ids,\n",
        "        \"query\": queries,\n",
        "        \"text\": texts,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_scifact_dataloader(\n",
        "    queries_path,\n",
        "    corpus_path,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "):\n",
        "    dataset = SciFactDataset(queries_path, corpus_path)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=scifact_collate_fn,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47752779",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6b55d1a0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"<s>\"\n",
        "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "\n",
        "def get_special_tokens_dict(tokenizer):\n",
        "    special_tokens_dict = {}\n",
        "    if tokenizer.pad_token is None:\n",
        "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
        "    if tokenizer.eos_token is None:\n",
        "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
        "    if tokenizer.bos_token is None:\n",
        "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
        "    if tokenizer.unk_token is None:\n",
        "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
        "    return special_tokens_dict\n",
        "\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict,\n",
        "    tokenizer,\n",
        "    model,\n",
        "):\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
        "            dim=0, keepdim=True\n",
        "        )\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
        "            dim=0, keepdim=True\n",
        "        )\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "\n",
        "def get_model_and_tokenizer(\n",
        "    model_name_or_path,\n",
        "    model_max_length=1024,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        model_max_length=model_max_length,\n",
        "        padding_side=\"left\",\n",
        "        use_fast=False,\n",
        "    )\n",
        "    special_tokens_dict = get_special_tokens_dict(tokenizer)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch_dtype,\n",
        "    )\n",
        "    smart_tokenizer_and_embedding_resize(\n",
        "        special_tokens_dict=special_tokens_dict,\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def get_genx_transformer(\n",
        "    query_model_name_or_path,\n",
        "    doc_model_name_or_path,\n",
        "    query_model_max_length=128,\n",
        "    doc_model_max_length=512,\n",
        "    num_beams: int = 5,\n",
        "    num_tokens: int = 5,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "):\n",
        "    query_model, query_tokenizer = get_model_and_tokenizer(\n",
        "        query_model_name_or_path,\n",
        "        model_max_length=query_model_max_length,\n",
        "        torch_dtype=torch_dtype,\n",
        "    )\n",
        "    doc_model, doc_tokenizer = get_model_and_tokenizer(\n",
        "        doc_model_name_or_path,\n",
        "        model_max_length=doc_model_max_length,\n",
        "        torch_dtype=torch_dtype,\n",
        "    )\n",
        "    return GenXTransformer(\n",
        "        query_model=query_model,\n",
        "        doc_model=doc_model,\n",
        "        query_tokenizer=query_tokenizer,\n",
        "        doc_tokenizer=doc_tokenizer,\n",
        "        num_beams=num_beams,\n",
        "        num_tokens=num_tokens,\n",
        "    )\n",
        "\n",
        "\n",
        "class GenXTransformer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        query_model,\n",
        "        doc_model,\n",
        "        query_tokenizer,\n",
        "        doc_tokenizer,\n",
        "        num_beams: int = 5,\n",
        "        num_tokens: int = 5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.query_model = query_model\n",
        "        self.doc_model = doc_model\n",
        "\n",
        "        self.query_tokenizer = query_tokenizer\n",
        "        self.doc_tokenizer = doc_tokenizer\n",
        "\n",
        "        self.num_beams = num_beams\n",
        "        self.num_tokens = num_tokens\n",
        "\n",
        "        self.verbose = False\n",
        "\n",
        "        self.config_genx_gen_kwargs(num_beams=num_beams, num_tokens=num_tokens)\n",
        "\n",
        "    def set_train_eval_mode(self, query_train: bool = True, doc_train: bool = False):\n",
        "        if query_train:\n",
        "            self.query_model.train()\n",
        "        else:\n",
        "            self.query_model.eval()\n",
        "        if doc_train:\n",
        "            self.doc_model.train()\n",
        "        else:\n",
        "            self.doc_model.eval()\n",
        "\n",
        "    def config_genx_gen_kwargs(self, **kwargs):\n",
        "        gen_kwargs = {\n",
        "            \"max_new_tokens\": kwargs.get(\"max_new_tokens\", 5),\n",
        "            \"do_sample\": False,\n",
        "            \"num_beams\": kwargs.get(\"num_beams\", 5),\n",
        "            \"num_return_sequences\": kwargs.get(\"num_return_sequences\", 5),\n",
        "            \"eos_token_id\": kwargs.get(\"eos_token_id\", None),\n",
        "            \"pad_token_id\": kwargs.get(\"pad_token_id\", None),\n",
        "        }\n",
        "        self.genx_gen_kwargs = gen_kwargs\n",
        "\n",
        "    def index_prompt(self, prompts, model, tokenizer):\n",
        "        device = model.device\n",
        "\n",
        "        if isinstance(prompts, str):\n",
        "            prompts = [prompts]\n",
        "\n",
        "        batch = tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "        )\n",
        "        batch[\"input_len\"] = len(batch[\"input_ids\"][0])\n",
        "\n",
        "        genx_gen_kwargs = self.genx_gen_kwargs.copy()\n",
        "        with torch.no_grad():\n",
        "            genx_gen_kwargs[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
        "            genx_gen_kwargs[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
        "            generated_tokens = model.generate(**genx_gen_kwargs)\n",
        "\n",
        "        input_len = batch[\"input_len\"]\n",
        "        pred_next_tokens = generated_tokens[:, input_len:]\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Decoded tokens:\",\n",
        "                tokenizer.batch_decode(\n",
        "                    pred_next_tokens, skip_special_tokens=False\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        batch_size = len(prompts)\n",
        "        num_return_sequences = genx_gen_kwargs[\"num_return_sequences\"]\n",
        "\n",
        "        pred_next_tokens = pred_next_tokens.view(batch_size, num_return_sequences, -1)\n",
        "        pred_next_tokens = pred_next_tokens.cpu().tolist()\n",
        "\n",
        "        print(\"Token IDs:\", pred_next_tokens) if self.verbose else None\n",
        "        return pred_next_tokens\n",
        "\n",
        "    def index_query(self, prompts: list[str]):\n",
        "        return self.index_prompt(prompts, self.query_model, self.query_tokenizer)\n",
        "\n",
        "    def index_doc(self, prompts: list[str]):\n",
        "        return self.index_prompt(prompts, self.doc_model, self.doc_tokenizer)\n",
        "\n",
        "    def sample_beams_of_next_tokens(\n",
        "        self,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        prompts: list[str],\n",
        "    ) -> list[list[str]]:\n",
        "        if isinstance(prompts, str):\n",
        "            prompts = [prompts]\n",
        "\n",
        "        device = model.device\n",
        "\n",
        "        batch = tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "        )\n",
        "        batch[\"input_len\"] = len(batch[\"input_ids\"][0])\n",
        "\n",
        "        gen_kwargs = self.genx_gen_kwargs.copy()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gen_kwargs[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
        "            gen_kwargs[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
        "            generated_tokens = model.generate(**gen_kwargs)\n",
        "\n",
        "        input_len = batch[\"input_len\"]\n",
        "        pred_next_tokens = generated_tokens[:, input_len:]\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Decoded tokens:\",\n",
        "                tokenizer.batch_decode(pred_next_tokens, skip_special_tokens=False),\n",
        "            )\n",
        "\n",
        "        batch_size = len(prompts)\n",
        "        num_return_sequences = gen_kwargs[\"num_return_sequences\"]\n",
        "\n",
        "        pred_next_tokens = pred_next_tokens.view(batch_size, num_return_sequences, -1)\n",
        "        pred_next_tokens = pred_next_tokens.cpu().tolist()\n",
        "\n",
        "        print(\"Token IDs:\", pred_next_tokens) if self.verbose else None\n",
        "        return pred_next_tokens\n",
        "\n",
        "    def get_sft_loss_txt(self, model, tokenizer, prompts: list[str]):\n",
        "        tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
        "\n",
        "        input_ids = tokens[\"input_ids\"]\n",
        "        attention_mask = tokens[\"attention_mask\"]\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        return loss\n",
        "\n",
        "    def __call__(self, queries: list[str], docs: list[str]):\n",
        "        # Now this is fine-tuning query model to generate next tokens of document\n",
        "        assert len(queries) == len(docs)\n",
        "\n",
        "        beams_for_docs: list[list[str]] = self.sample_beams_of_next_tokens(\n",
        "            self.doc_model,\n",
        "            self.doc_tokenizer,\n",
        "            docs,\n",
        "        )  # Shape is num_docs x num_beams x num_tokens\n",
        "\n",
        "        # Shape is num_docs x num_beams x (len(query) + num_tokens)\n",
        "        prompts_for_all_pairs: list[list[str]] = []\n",
        "        for doc_idx, beams in enumerate(beams_for_docs):\n",
        "            beams = self.doc_tokenizer.batch_decode(beams, skip_special_tokens=False)\n",
        "            prompts = []  # List of the same query and num_beams possible next sentences\n",
        "\n",
        "            query = queries[doc_idx]\n",
        "            num_beams = len(beams)\n",
        "            for beams_idx in range(num_beams):\n",
        "                prompt = query + beams[beams_idx]\n",
        "                prompts.append(prompt)\n",
        "\n",
        "            prompts_for_all_pairs.append(prompts)\n",
        "\n",
        "        # Have num_docs x num_beams sequences, each of a string of length (len(query) + num_tokens)\n",
        "        flats: list[str] = list(itertools.chain.from_iterable(prompts_for_all_pairs))\n",
        "\n",
        "        loss = self.get_sft_loss_txt(self.query_model, self.query_tokenizer, flats)\n",
        "        return loss\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cece13a5",
      "metadata": {},
      "source": [
        "## Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f202b0bb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, text, metadata):\n",
        "        self._text = text\n",
        "        self._metadata = metadata\n",
        "\n",
        "    def get_text(self):\n",
        "        return self._text\n",
        "\n",
        "    def get_metadata(self):\n",
        "        return self._metadata\n",
        "\n",
        "\n",
        "class IndexStoreTemplate(ABC):\n",
        "    def __init__(self, initial_capacity=1000):\n",
        "        # Capacity and initial capacity\n",
        "        self._initial_capacity = initial_capacity\n",
        "        self.capacity = initial_capacity\n",
        "\n",
        "        # Data and global index of elements\n",
        "        self.next_id = 0\n",
        "        self.size = 0\n",
        "        self._ids = np.zeros(self.capacity, dtype=np.int64)\n",
        "        self._data_store = {}  # Dictionary to store actual data\n",
        "\n",
        "    def _resize_if_needed(self, additional_items=16):\n",
        "        if self.size + additional_items > self.capacity:\n",
        "            new_capacity = max(self.capacity * 2, self.size + additional_items)\n",
        "\n",
        "            # Resize ID array\n",
        "            new_ids = np.zeros(new_capacity, dtype=np.int64)\n",
        "            new_ids[: self.size] = self._ids[: self.size]\n",
        "            self._ids = new_ids\n",
        "\n",
        "            self.capacity = new_capacity\n",
        "\n",
        "    def _clear_store(self):\n",
        "        self.capacity = self._initial_capacity\n",
        "        self.next_id = 0\n",
        "        self.size = 0\n",
        "        self._ids = np.zeros(self._initial_capacity, dtype=np.int64)\n",
        "        self._data_store = {}\n",
        "\n",
        "    def retrieve(self, doc_id):\n",
        "        return self._data_store[doc_id]\n",
        "\n",
        "    @abstractmethod\n",
        "    def insert(self, text: list[Document]):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def query(self, query_text: Document) -> list[list[int]]:\n",
        "        pass\n",
        "\n",
        "\n",
        "class PrefixTreeNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.doc_ids = set()\n",
        "\n",
        "\n",
        "class Prompt:\n",
        "    def __init__(self, before, after):\n",
        "        self.before = before\n",
        "        self.after = after\n",
        "\n",
        "    def template(self, text):\n",
        "        return self.before + text + self.after\n",
        "\n",
        "\n",
        "class SequencePrefixTreeIndexStore(IndexStoreTemplate):\n",
        "    def __init__(\n",
        "        self,\n",
        "        transformer,\n",
        "        id_len,\n",
        "        universe,\n",
        "        doc_prompt_before=\"Generate identifying phrases that memorize the key concepts in this text. You are not supposed to make sense. Just generate ONLY the identifying phrases without any punctuations or numbers before or after. \",\n",
        "        doc_prompt_after=\" IGNORE ME. Phrases: \",\n",
        "        query_prompt_before=\"From this query create identifying phrases that capture the key concepts and align with phrases found in relevant text. Do not aim for meaningful sentences. Only output the identifying phrases with no punctuation or numbers before or after. \",\n",
        "        query_prompt_after=\" IGNORE ME. Phrases: \",\n",
        "        duplicate_prompt_before=\"Given the text first remove all the punctuations and stop words. Then shuffle the sentences. Generate some unique related phrases that does not have synonyms. \",\n",
        "        duplicate_prompt_after=\" IGNORE ME. Phrases: \",\n",
        "        verbose=False,\n",
        "        initial_capacity=1000,\n",
        "        insertion_depth=3,\n",
        "        mode=\"document_search\",\n",
        "    ):\n",
        "        super().__init__(initial_capacity)\n",
        "\n",
        "        assert mode in [\"duplicate_detection\", \"document_search\"]\n",
        "        self.mode = mode\n",
        "        if mode == \"duplicate_detection\":\n",
        "            self.doc_prompt = Prompt(duplicate_prompt_before, duplicate_prompt_after)\n",
        "            self.query_prompt = Prompt(duplicate_prompt_before, duplicate_prompt_after)\n",
        "        elif mode == \"document_search\":\n",
        "            self.doc_prompt = Prompt(doc_prompt_before, doc_prompt_after)\n",
        "            self.query_prompt = Prompt(query_prompt_before, query_prompt_after)\n",
        "\n",
        "        self.doc_prompt_before = doc_prompt_before\n",
        "        self.doc_prompt_after = doc_prompt_after\n",
        "        self.query_prompt_before = query_prompt_before\n",
        "        self.query_prompt_after = query_prompt_after\n",
        "        self.duplicate_prompt_before = duplicate_prompt_before\n",
        "        self.duplicate_prompt_after = duplicate_prompt_after\n",
        "\n",
        "        # Model for generating indices for inserted documens\n",
        "        self.transformer = transformer\n",
        "        self.id_len = id_len\n",
        "        self.universe = set(universe)\n",
        "\n",
        "        # Verbose\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Prefix tree\n",
        "        self.root = PrefixTreeNode()\n",
        "        self.insertion_depth = insertion_depth\n",
        "\n",
        "    def set_verbose_for_all(self, verbose):\n",
        "        self.verbose = verbose\n",
        "        if hasattr(self.transformer, \"verbose\"):\n",
        "            self.transformer.verbose = verbose\n",
        "\n",
        "    def reset_id_len(self, id_len):\n",
        "        self.id_len = id_len\n",
        "        self.transformer.update_gen_kwargs(max_new_tokens=id_len)\n",
        "\n",
        "    def set_mode(self, mode):\n",
        "        assert mode in [\"duplicate_detection\", \"document_search\"]\n",
        "        self.mode = mode\n",
        "        if mode == \"duplicate_detection\":\n",
        "            self.doc_prompt = Prompt(\n",
        "                self.duplicate_prompt_before, self.duplicate_prompt_after\n",
        "            )\n",
        "            self.query_prompt = Prompt(\n",
        "                self.duplicate_prompt_before, self.duplicate_prompt_after\n",
        "            )\n",
        "        elif mode == \"document_search\":\n",
        "            self.doc_prompt = Prompt(self.doc_prompt_before, self.doc_prompt_after)\n",
        "            self.query_prompt = Prompt(\n",
        "                self.query_prompt_before, self.query_prompt_after\n",
        "            )\n",
        "        print(\"Remember to call `clear_store` to reset the database!\")\n",
        "\n",
        "    def clear_store(self):\n",
        "        self.root = PrefixTreeNode()\n",
        "\n",
        "        super()._clear_store()\n",
        "        if self.verbose:\n",
        "            print(f\"Store cleared, current capacity: {self.capacity}\")\n",
        "\n",
        "    def _insert_document(self, texts: list[Document], prompt_template):\n",
        "        if not isinstance(texts, list):\n",
        "            texts = [texts]\n",
        "\n",
        "        self._resize_if_needed(len(texts))\n",
        "\n",
        "        doc_ids = []\n",
        "        template_texts = []\n",
        "        for text in texts:\n",
        "            doc_id = self.next_id\n",
        "            doc_ids.append(doc_id)\n",
        "            # Update index in data store\n",
        "            self.next_id += 1\n",
        "            self.size += 1\n",
        "\n",
        "            # Save text in data store\n",
        "            self._ids[self.size - 1] = doc_id\n",
        "            self._data_store[doc_id] = text\n",
        "\n",
        "            template_text = prompt_template(text.get_text())\n",
        "            template_texts.append(template_text)\n",
        "\n",
        "        # Generate beams of sequences\n",
        "        # [batch_size, num_return_sequences, sequence_length]\n",
        "        lst_of_sequences = self.transformer.index_doc(template_texts)\n",
        "        print(lst_of_sequences) if self.verbose else None\n",
        "        self._insert_sequences_into_tree(lst_of_sequences, doc_ids)\n",
        "\n",
        "    def _insert_sequences_into_tree(self, lst_of_sequences: list[list[list[int]]], doc_ids: list[int]):\n",
        "        for sequences, doc_id in zip(lst_of_sequences, doc_ids):\n",
        "            for seq in sequences:\n",
        "                print(f\"Tokens: {seq}\") if self.verbose else None\n",
        "                if len(seq) != self.id_len or not all(x in self.universe for x in seq):\n",
        "                    continue  # Skip invalid sequences\n",
        "\n",
        "                self._traverse_and_insert(seq, doc_id)\n",
        "\n",
        "    def _traverse_and_insert(self, seq, doc_id):\n",
        "        node = self.root\n",
        "        depth = 0\n",
        "\n",
        "        for idx in seq:\n",
        "            if idx not in node.children:\n",
        "                node.children[idx] = PrefixTreeNode()\n",
        "            node = node.children[idx]\n",
        "            depth += 1\n",
        "            if depth >= self.insertion_depth:\n",
        "                node.doc_ids.add(doc_id)\n",
        "                if self.verbose:\n",
        "                    print(f\"Inserted doc {doc_id} at depth {depth} of prefix tree\")\n",
        "\n",
        "    def insert(self, texts: list[Document]):\n",
        "        print(f\"Inserting '{texts}'\") if self.verbose else None\n",
        "        self._insert_document(texts, self.doc_prompt.template)\n",
        "\n",
        "    def _query_with_prompt(self, query_texts: list[Document], prompt_template):\n",
        "        if not query_texts:\n",
        "            return []\n",
        "\n",
        "        lst_of_result_ids = []\n",
        "        template_texts = []\n",
        "        for query_text in query_texts:\n",
        "            template_text = prompt_template(query_text.get_text())\n",
        "            print(template_text) if self.verbose else None\n",
        "            template_texts.append(template_text)\n",
        "\n",
        "        # [batch_size, num_return_sequences, sequence_length]\n",
        "        lst_of_sequences = self.transformer.index_query(template_texts)\n",
        "        for sequences in lst_of_sequences:\n",
        "            result_ids = []\n",
        "            for seq in sequences:\n",
        "                print(f\"Tokens: {seq}\") if self.verbose else None\n",
        "                if len(seq) != self.id_len or not all(x in self.universe for x in seq):\n",
        "                    continue\n",
        "\n",
        "                result = self._traverse_tree_for_query(seq)\n",
        "                if result:\n",
        "                    result[\"index_ids\"] = seq\n",
        "                    result[\"index_txt\"] = self.transformer.doc_tokenizer.batch_decode(seq, skip_special_tokens=False)\n",
        "                    result_ids.append(result)\n",
        "            print(\"Found results: \", result_ids) if self.verbose else None\n",
        "            lst_of_result_ids.append(result_ids)\n",
        "\n",
        "        return lst_of_result_ids\n",
        "\n",
        "    def _traverse_tree_for_query(self, seq):\n",
        "        node: PrefixTreeNode = self.root\n",
        "        found = True\n",
        "        depth = 0\n",
        "\n",
        "        for idx in seq:\n",
        "            if (idx not in node.children) and (depth < self.insertion_depth):\n",
        "                found = False\n",
        "                break\n",
        "            if (idx not in node.children) and (depth >= self.insertion_depth):\n",
        "                break\n",
        "            node = node.children[idx]\n",
        "            depth += 1\n",
        "\n",
        "        if found:\n",
        "            return {\"depth\": depth, \"doc_ids\": node.doc_ids}\n",
        "        return None\n",
        "\n",
        "    def query(self, query_texts: list[Document]):\n",
        "        print(f\"Querying for '{query_texts}'\") if self.verbose else None\n",
        "        return self._query_with_prompt(query_texts, self.query_prompt.template)\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "939615b3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        }
      ],
      "source": [
        "query_model_name_or_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "query_model_max_length = 128\n",
        "doc_model_name_or_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "doc_model_max_length = 2048\n",
        "torch_dtype = torch.bfloat16\n",
        "\n",
        "query_model, query_tokenizer = get_model_and_tokenizer(\n",
        "    query_model_name_or_path,\n",
        "    model_max_length=query_model_max_length,\n",
        "    torch_dtype=torch_dtype,\n",
        ")\n",
        "doc_model, doc_tokenizer = get_model_and_tokenizer(\n",
        "    doc_model_name_or_path,\n",
        "    model_max_length=doc_model_max_length,\n",
        "    torch_dtype=torch_dtype,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d2001e65",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128258, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128258, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_model.to(\"cuda\")\n",
        "doc_model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9dc06290",
      "metadata": {},
      "outputs": [],
      "source": [
        "genx_transformer = GenXTransformer(\n",
        "    query_model,\n",
        "    doc_model,\n",
        "    query_tokenizer,\n",
        "    doc_tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9d253ce6",
      "metadata": {},
      "outputs": [],
      "source": [
        "store = SequencePrefixTreeIndexStore(\n",
        "    genx_transformer,\n",
        "    id_len=5,\n",
        "    universe=set(range(genx_transformer.doc_tokenizer.vocab_size)),\n",
        "    mode=\"document_search\",\n",
        "    insertion_depth=4,\n",
        ")\n",
        "store.clear_store()\n",
        "store.set_verbose_for_all(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bc2eb53d",
      "metadata": {},
      "outputs": [],
      "source": [
        "scifact_train_dataloader = get_scifact_dataloader(\n",
        "    \"./data/scifact/claims_train.jsonl\",\n",
        "    \"./data/scifact/corpus.jsonl\",\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "252608e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "for batch in scifact_train_dataloader:\n",
        "    # Use dict to automatically handle duplicates (keeps last occurrence)\n",
        "    unique_docs = {}\n",
        "    for doc_id, text in zip(batch[\"doc_id\"], batch[\"text\"]):\n",
        "        unique_docs[doc_id] = text\n",
        "\n",
        "    docs_to_be_inserted = []\n",
        "    for doc_id, text in unique_docs.items():\n",
        "        docs_to_be_inserted.append(Document(text, {\"doc_id\": doc_id}))\n",
        "\n",
        "    store.insert(docs_to_be_inserted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "8c892df5",
      "metadata": {},
      "outputs": [],
      "source": [
        "cited_doc_ids = []\n",
        "results = []\n",
        "for batch in scifact_train_dataloader:\n",
        "    # Use dict to automatically handle duplicates (keeps last occurrence)\n",
        "    unique_queries = {}\n",
        "    for query, doc_id in zip(batch[\"query\"], batch[\"doc_id\"]):\n",
        "        if query not in unique_queries:\n",
        "            unique_queries[query] = []\n",
        "        unique_queries[query].append(doc_id)\n",
        "\n",
        "    queries_to_be_queried = []\n",
        "    doc_ids_for_a_query = []\n",
        "    for query, doc_ids in unique_queries.items():\n",
        "        queries_to_be_queried.append(Document(query, {\"doc_ids\": doc_ids}))\n",
        "        doc_ids_for_a_query.append(doc_ids)\n",
        "\n",
        "    result = store.query(queries_to_be_queried)\n",
        "    results.extend(result)\n",
        "    cited_doc_ids.extend(doc_ids_for_a_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "7629a928",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(807, 807)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(results), len(cited_doc_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "7f62bbbb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([], [31715818])"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results[0], cited_doc_ids[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "61cb2704",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total predicted abstracts: 5238\n",
            "Total correctly predicted abstracts: 0\n",
            "Total gold abstracts: 919\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1 Score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Count predicted abstracts, correctly predicted abstracts, and gold abstracts\n",
        "total_predicted = 0\n",
        "total_correctly_predicted = 0\n",
        "total_gold = 0\n",
        "\n",
        "for idx in range(len(results)):\n",
        "    result = results[idx]\n",
        "    cited_doc_id = cited_doc_ids[idx]\n",
        "\n",
        "    # Extract predicted document IDs\n",
        "    if len(result) > 0:\n",
        "        temp = []\n",
        "        for item in result:\n",
        "            temp.extend(item['doc_ids'])\n",
        "        predicted = set(temp)\n",
        "    else:\n",
        "        predicted = set()\n",
        "\n",
        "    # Count metrics\n",
        "    total_predicted += len(predicted)\n",
        "    total_gold += len(cited_doc_id)\n",
        "\n",
        "    # Count correctly predicted abstracts (intersection)\n",
        "    correctly_predicted = predicted.intersection(set(cited_doc_id))\n",
        "    total_correctly_predicted += len(correctly_predicted)\n",
        "\n",
        "print(f\"Total predicted abstracts: {total_predicted}\")\n",
        "print(f\"Total correctly predicted abstracts: {total_correctly_predicted}\")\n",
        "print(f\"Total gold abstracts: {total_gold}\")\n",
        "\n",
        "# Calculate precision and recall\n",
        "if total_predicted > 0:\n",
        "    precision = total_correctly_predicted / total_predicted\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "else:\n",
        "    print(\"Precision: 0.0000 (no predictions)\")\n",
        "\n",
        "if total_gold > 0:\n",
        "    recall = total_correctly_predicted / total_gold\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "else:\n",
        "    print(\"Recall: 0.0000 (no gold abstracts)\")\n",
        "\n",
        "# Calculate F1 score\n",
        "if precision + recall > 0:\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "else:\n",
        "    print(\"F1 Score: 0.0000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "92c5a93e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://github.com/allenai/scifact/blob/master/doc/evaluation.md#abstract-level-scoring\n",
        "\n",
        "# Precision: (# correctly predicted abstracts) / (# predicted abstracts)\n",
        "# Recall: (# correctly predicted abstracts) / (# gold abstracts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5044d721",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "f6f5166f-c2aa-4d71-aca2-8be25846c691",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "df",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
