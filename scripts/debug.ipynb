{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import transformers.utils.logging as transformers_utils_logging\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import (\n",
        "    DistributedDataParallelKwargs,\n",
        "    ProjectConfiguration,\n",
        "    set_seed,\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    get_scheduler,\n",
        "    HfArgumentParser,\n",
        "    PreTrainedTokenizer,\n",
        ")\n",
        "\n",
        "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "\n",
        "TRANSFORMERS_PATH_MAP = {\n",
        "    \"llama-3.2-1b-instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    \"llama-3.1-8b-instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################## Datset ########################################\n",
        "\n",
        "\n",
        "def row2text_template_scifact(row):\n",
        "    text = f\"Title: {row['title']}\\nAbstract: {' '.join(row['abstract'])}\\nStructured: {row['structured']}\\n\"\n",
        "    # Remove all newlines and strip extra spaces\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    return text\n",
        "\n",
        "\n",
        "class SciFactCorpusDataset(Dataset):\n",
        "    def __init__(self, corpus_path):\n",
        "        super().__init__()\n",
        "        corpus = pd.read_json(corpus_path, lines=True)\n",
        "        corpus[\"text\"] = corpus.apply(row2text_template_scifact, axis=1)\n",
        "\n",
        "        self.corpus = corpus\n",
        "        self.corpus_dict = corpus.set_index(\"doc_id\")[\"text\"].to_dict()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        doc_id = self.corpus[\"doc_id\"][i]\n",
        "        text = self.corpus_dict.get(doc_id)\n",
        "        return {\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": text,\n",
        "        }\n",
        "\n",
        "    def get_corpus_dict(self):\n",
        "        return self.corpus_dict\n",
        "\n",
        "\n",
        "def get_scifact_corpus_dataloader(\n",
        "    corpus_path,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "):\n",
        "    scifact_corpus_dataset = SciFactCorpusDataset(corpus_path)\n",
        "    dataloader = DataLoader(\n",
        "        scifact_corpus_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "class SciFactQueryDataset(Dataset):\n",
        "    def __init__(self, queries_path, corpus_dict):\n",
        "        super().__init__()\n",
        "        if isinstance(queries_path, str):\n",
        "            self.queries = pd.read_json(queries_path, lines=True)\n",
        "        elif isinstance(queries_path, list):\n",
        "            queries = []\n",
        "            for path in queries_path:\n",
        "                queries.append(pd.read_json(path, lines=True))\n",
        "            self.queries = pd.concat(queries, ignore_index=True)\n",
        "        else:\n",
        "            raise ValueError(\"queries_path must be a string or a list of strings\")\n",
        "\n",
        "        self.corpus_dict = corpus_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        doc_id_list = self.queries[\"cited_doc_ids\"][i]\n",
        "        query = self.queries[\"claim\"][i]\n",
        "\n",
        "        docs = []\n",
        "        for doc_id in doc_id_list:\n",
        "            docs.append(self.corpus_dict.get(doc_id))\n",
        "\n",
        "        n_docs = len(docs)\n",
        "\n",
        "        if n_docs == 0:\n",
        "            return {}\n",
        "        else:\n",
        "            queries = [query] * n_docs\n",
        "\n",
        "        return {\n",
        "            \"doc_id\": doc_id_list,\n",
        "            \"query\": queries,\n",
        "            \"text\": docs,\n",
        "        }\n",
        "\n",
        "\n",
        "def scifact_query_collate_fn(batch):\n",
        "    batch = [item for item in batch if item]\n",
        "    if not batch:\n",
        "        return {}\n",
        "    doc_ids = sum([item[\"doc_id\"] for item in batch], [])\n",
        "    queries = sum([item[\"query\"] for item in batch], [])\n",
        "    texts = sum([item[\"text\"] for item in batch], [])\n",
        "    return {\n",
        "        \"doc_id\": doc_ids,\n",
        "        \"query\": queries,\n",
        "        \"text\": texts,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_scifact_query_dataloader(\n",
        "    queries_path: str | list[str],\n",
        "    corpus_dict,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "):\n",
        "    scifact_query_dataset = SciFactQueryDataset(\n",
        "        queries_path,\n",
        "        corpus_dict,\n",
        "    )\n",
        "    dataloader = DataLoader(\n",
        "        scifact_query_dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=scifact_query_collate_fn,  # NOTE: This is for query dataset only\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################## Model ########################################\n",
        "\n",
        "\n",
        "def get_special_tokens_dict(tokenizer):\n",
        "    special_tokens_dict = {}\n",
        "    if tokenizer.pad_token is None:\n",
        "        special_tokens_dict[\"pad_token\"] = \"[PAD]\"\n",
        "    if tokenizer.eos_token is None:\n",
        "        special_tokens_dict[\"eos_token\"] = \"</s>\"\n",
        "    if tokenizer.bos_token is None:\n",
        "        special_tokens_dict[\"bos_token\"] = \"<s>\"\n",
        "    if tokenizer.unk_token is None:\n",
        "        special_tokens_dict[\"unk_token\"] = \"<unk>\"\n",
        "    return special_tokens_dict\n",
        "\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict,\n",
        "    tokenizer,\n",
        "    model,\n",
        "):\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
        "            dim=0, keepdim=True\n",
        "        )\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
        "            dim=0, keepdim=True\n",
        "        )\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "\n",
        "def _tokenize_fn(strings: list[str], tokenizer: PreTrainedTokenizer):\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
        "        for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_model_and_tokenizer(\n",
        "    model_name_or_path,\n",
        "    model_max_length=1024,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        model_max_length=model_max_length,\n",
        "        padding_side=\"right\",\n",
        "        use_fast=False,\n",
        "    )\n",
        "    special_tokens_dict = get_special_tokens_dict(tokenizer)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch_dtype,\n",
        "    )\n",
        "    smart_tokenizer_and_embedding_resize(\n",
        "        special_tokens_dict=special_tokens_dict,\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "class GenXTransformer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        query_model,\n",
        "        doc_model,\n",
        "        query_tokenizer,\n",
        "        doc_tokenizer,\n",
        "        num_beams: int = 5,\n",
        "        num_next_tokens: int = 5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.query_model = query_model\n",
        "        self.doc_model = doc_model\n",
        "\n",
        "        self.query_tokenizer = query_tokenizer\n",
        "        self.doc_tokenizer = doc_tokenizer\n",
        "\n",
        "        self.num_beams = num_beams\n",
        "        self.num_next_tokens = num_next_tokens\n",
        "\n",
        "        self.verbose = False\n",
        "\n",
        "        self.config_genx_gen_kwargs(\n",
        "            num_beams=num_beams,\n",
        "            num_return_sequences=num_beams,\n",
        "            max_new_tokens=num_next_tokens,\n",
        "        )\n",
        "\n",
        "    def set_train_eval_mode(self, query_train: bool = True, doc_train: bool = False):\n",
        "        if query_train:\n",
        "            self.query_model.train()\n",
        "        else:\n",
        "            self.query_model.eval()\n",
        "        if doc_train:\n",
        "            self.doc_model.train()\n",
        "        else:\n",
        "            self.doc_model.eval()\n",
        "\n",
        "    def update_num_next_tokens(self, num_next_tokens: int):\n",
        "        self.num_next_tokens = num_next_tokens\n",
        "        self.genx_gen_kwargs[\"max_new_tokens\"] = num_next_tokens\n",
        "\n",
        "    def config_genx_gen_kwargs(self, **kwargs):\n",
        "        gen_kwargs = {\n",
        "            \"max_new_tokens\": kwargs.get(\"max_new_tokens\", 5),\n",
        "            \"do_sample\": False,\n",
        "            \"num_beams\": kwargs.get(\"num_beams\", 1),\n",
        "            \"num_return_sequences\": kwargs.get(\"num_return_sequences\", 1),\n",
        "            \"eos_token_id\": kwargs.get(\"eos_token_id\", None),\n",
        "        }\n",
        "        self.genx_gen_kwargs = gen_kwargs\n",
        "\n",
        "    def index_prompt(self, prompts, model, tokenizer, shift=0):\n",
        "        device = model.device\n",
        "\n",
        "        if isinstance(prompts, str):\n",
        "            prompts = [prompts]\n",
        "\n",
        "        batch = tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "        )\n",
        "\n",
        "        genx_gen_kwargs = self.genx_gen_kwargs.copy()\n",
        "        genx_gen_kwargs[\"max_new_tokens\"] += shift\n",
        "\n",
        "        bad_chars = [\"#\", \"\\n\", \".\", \"}\", \"{\", \"step\"] + list(\"0123456789\")\n",
        "        bad_words_ids = [\n",
        "            tokenizer.encode(char, add_special_tokens=False) for char in bad_chars\n",
        "        ]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            genx_gen_kwargs[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
        "            genx_gen_kwargs[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
        "            generated_tokens = model.generate(\n",
        "                **genx_gen_kwargs,\n",
        "                bad_words_ids=bad_words_ids,\n",
        "            )\n",
        "\n",
        "        input_len = len(batch[\"input_ids\"][0])\n",
        "        pred_next_tokens = generated_tokens[:, input_len + shift :]\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Decoded tokens:\",\n",
        "                tokenizer.batch_decode(pred_next_tokens, skip_special_tokens=False),\n",
        "            )\n",
        "\n",
        "        batch_size = len(prompts)\n",
        "        num_return_sequences = genx_gen_kwargs[\"num_return_sequences\"]\n",
        "\n",
        "        pred_next_tokens = pred_next_tokens.view(batch_size, num_return_sequences, -1)\n",
        "        pred_next_tokens = pred_next_tokens.cpu().tolist()\n",
        "\n",
        "        print(\"Token IDs:\", pred_next_tokens) if self.verbose else None\n",
        "        return pred_next_tokens\n",
        "\n",
        "    def index_query(self, prompts: list[str]):\n",
        "        return self.index_prompt(prompts, self.query_model, self.query_tokenizer, 0)\n",
        "\n",
        "    def index_doc(self, prompts: list[str]):\n",
        "        return self.index_prompt(prompts, self.doc_model, self.doc_tokenizer, 3)\n",
        "\n",
        "    def get_sft_loss_txt(self, model, tokenizer, prompts: list[str]):\n",
        "        tokens = tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=tokenizer.model_max_length,\n",
        "        ).to(model.device)\n",
        "\n",
        "        input_ids = tokens[\"input_ids\"]\n",
        "        attention_mask = tokens[\"attention_mask\"]\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        keep = self.num_next_tokens\n",
        "        for i, mask in enumerate(attention_mask):\n",
        "            length = mask.sum().item()\n",
        "            cutoff = max(0, length - keep)\n",
        "            labels[i, :cutoff] = IGNORE_INDEX\n",
        "            labels[i, length:] = IGNORE_INDEX\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "        return outputs.loss\n",
        "\n",
        "    def __call__(self, queries: list[str], docs: list[str]):\n",
        "        # Now this is fine-tuning query model to generate next tokens of document\n",
        "        assert len(queries) == len(docs)\n",
        "\n",
        "        if isinstance(docs[0], str):\n",
        "            beams_for_docs: list[list[list[int]]] = self.index_doc(\n",
        "                docs,\n",
        "            )  # Shape is num_docs x num_beams x num_next_tokens\n",
        "        else:\n",
        "            beams_for_docs = docs\n",
        "\n",
        "        # Shape is num_docs x num_beams x (len(query) + num_next_tokens)\n",
        "        prompts_for_all_pairs: list[list[str]] = []\n",
        "        for doc_idx, beams in enumerate(beams_for_docs):\n",
        "            beams = self.doc_tokenizer.batch_decode(beams, skip_special_tokens=False)\n",
        "            prompts = []  # List of the same query and num_beams possible next sentences\n",
        "\n",
        "            query = queries[doc_idx]\n",
        "            num_beams = len(beams)\n",
        "            for beams_idx in range(num_beams):\n",
        "                prompt = query + beams[beams_idx]\n",
        "                prompts.append(prompt)\n",
        "            prompts_for_all_pairs.append(prompts)\n",
        "\n",
        "        # Have num_docs x num_beams sequences, each of a string of length (len(query) + num_next_tokens)\n",
        "        flats: list[str] = list(itertools.chain.from_iterable(prompts_for_all_pairs))\n",
        "        loss = self.get_sft_loss_txt(self.query_model, self.query_tokenizer, flats)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################## Store ########################################\n",
        "\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, text, metadata):\n",
        "        self._text = text\n",
        "        self._metadata = metadata\n",
        "\n",
        "    def get_text(self):\n",
        "        return self._text\n",
        "\n",
        "    def get_metadata(self):\n",
        "        return self._metadata\n",
        "\n",
        "\n",
        "class IndexStoreTemplate(ABC):\n",
        "    def __init__(self, initial_capacity=1000):\n",
        "        # Capacity and initial capacity\n",
        "        self._initial_capacity = initial_capacity\n",
        "        self.capacity = initial_capacity\n",
        "\n",
        "        # Data and global index of elements\n",
        "        self.next_id = 0\n",
        "        self.size = 0\n",
        "        self._ids = np.zeros(self.capacity, dtype=np.int64)\n",
        "        self._data_store = {}  # id to text\n",
        "        self._beams_store = {}  # id to beams\n",
        "        self._original_keys = {}  # id to original key in the dataset\n",
        "        self._original_keys_transpose = {}\n",
        "\n",
        "    def _resize_if_needed(self, additional_items=16):\n",
        "        if self.size + additional_items > self.capacity:\n",
        "            new_capacity = max(self.capacity * 2, self.size + additional_items)\n",
        "\n",
        "            # Resize ID array\n",
        "            new_ids = np.zeros(new_capacity, dtype=np.int64)\n",
        "            new_ids[: self.size] = self._ids[: self.size]\n",
        "            self._ids = new_ids\n",
        "\n",
        "            self.capacity = new_capacity\n",
        "\n",
        "    def _clear_store(self):\n",
        "        self.capacity = self._initial_capacity\n",
        "        self.next_id = 0\n",
        "        self.size = 0\n",
        "        self._ids = np.zeros(self._initial_capacity, dtype=np.int64)\n",
        "        self._data_store = {}\n",
        "        self._beams_store = {}\n",
        "        self._original_keys = {}\n",
        "        self._original_keys_transpose = {}\n",
        "\n",
        "    def retrieve(self, doc_id):\n",
        "        return self._data_store[doc_id]\n",
        "\n",
        "    @abstractmethod\n",
        "    def insert(self, text: list[Document]):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def query(self, query_text: Document) -> list[list[int]]:\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_list_frequencies(\n",
        "        data,\n",
        "        figsize=(12, 8),\n",
        "        show_top_n=10,\n",
        "        save_path=None,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        # Extract all lists and convert to tuples for counting\n",
        "        all_lists = []\n",
        "        for list_of_lists in data.values():\n",
        "            for sublist in list_of_lists:\n",
        "                all_lists.append(tuple(sublist))\n",
        "\n",
        "        if not all_lists:\n",
        "            print(\"No data found in the input dictionary.\")\n",
        "            return {}\n",
        "\n",
        "        # Count frequencies of each unique list\n",
        "        list_freq = Counter(all_lists)\n",
        "\n",
        "        # Count the frequency of frequencies\n",
        "        freq_of_freq = Counter(list_freq.values())\n",
        "\n",
        "        # Create the figure and axes\n",
        "        fig, axes = plt.subplots(2, 1, figsize=figsize)\n",
        "\n",
        "        # Plot 1: Frequency of frequencies\n",
        "        frequencies = sorted(freq_of_freq.keys())\n",
        "        counts = [freq_of_freq[f] for f in frequencies]\n",
        "\n",
        "        axes[0].bar(\n",
        "            frequencies, counts, color=\"lightblue\", edgecolor=\"black\", alpha=0.7\n",
        "        )\n",
        "        axes[0].set_xlabel(\"ID Count\", fontsize=12)\n",
        "        axes[0].set_ylabel(\"Number of lists\", fontsize=12)\n",
        "        axes[0].set_title(\"Distribution of List Frequencies\", fontsize=14)\n",
        "        axes[0].grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "        axes[0].set_xticks(frequencies)\n",
        "\n",
        "        # Plot 2: Histogram of all frequency values\n",
        "        all_frequency_values = list(list_freq.values())\n",
        "        max_freq = max(all_frequency_values)\n",
        "        axes[1].hist(\n",
        "            all_frequency_values,\n",
        "            bins=range(1, max_freq + 2),\n",
        "            rwidth=0.8,\n",
        "            align=\"left\",\n",
        "            color=\"lightcoral\",\n",
        "            edgecolor=\"black\",\n",
        "            alpha=0.7,\n",
        "        )\n",
        "        axes[1].set_xlabel(\"Frequency\", fontsize=12)\n",
        "        axes[1].set_ylabel(\"Count\", fontsize=12)\n",
        "        axes[1].set_title(\"Histogram of List Frequencies\", fontsize=14)\n",
        "        axes[1].grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "        axes[1].set_xticks(range(1, max_freq + 1))\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save or show the plot\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "            print(f\"Plot saved to {save_path}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "        if verbose:\n",
        "            # Calculate statistics\n",
        "            stats = {\n",
        "                \"total_lists\": len(all_lists),\n",
        "                \"unique_lists\": len(list_freq),\n",
        "                \"lists_appearing_once\": freq_of_freq.get(1, 0),\n",
        "                \"lists_appearing_multiple\": sum(\n",
        "                    count for freq, count in freq_of_freq.items() if freq > 1\n",
        "                ),\n",
        "                \"max_frequency\": max(all_frequency_values),\n",
        "                \"frequency_distribution\": dict(freq_of_freq),\n",
        "            }\n",
        "\n",
        "            # Print statistics\n",
        "            print(\"Summary Statistics:\")\n",
        "            print(f\"Total number of lists: {stats['total_lists']}\")\n",
        "            print(f\"Number of unique lists: {stats['unique_lists']}\")\n",
        "            print(f\"Lists appearing once: {stats['lists_appearing_once']}\")\n",
        "            print(\n",
        "                f\"Lists appearing more than once: {stats['lists_appearing_multiple']}\"\n",
        "            )\n",
        "            print(f\"Maximum frequency: {stats['max_frequency']}\")\n",
        "\n",
        "            # Show top frequent lists\n",
        "            if len(list_freq) <= show_top_n * 2:\n",
        "                print(\"\\nAll unique lists and their frequencies:\")\n",
        "                for i, (unique_list, count) in enumerate(\n",
        "                    sorted(list_freq.items(), key=lambda x: x[1], reverse=True), 1\n",
        "                ):\n",
        "                    print(f\"{i:2d}: {list(unique_list)} appears {count} time(s)\")\n",
        "            else:\n",
        "                print(f\"\\nTop {show_top_n} most frequent lists:\")\n",
        "                for i, (unique_list, count) in enumerate(\n",
        "                    sorted(list_freq.items(), key=lambda x: x[1], reverse=True)[\n",
        "                        :show_top_n\n",
        "                    ],\n",
        "                    1,\n",
        "                ):\n",
        "                    print(f\"{i:2d}: {list(unique_list)} appears {count} time(s)\")\n",
        "\n",
        "            print(\"\\nFrequency distribution:\")\n",
        "            for freq in sorted(freq_of_freq.keys()):\n",
        "                print(f\"{freq_of_freq[freq]} lists appear exactly {freq} time(s)\")\n",
        "\n",
        "            return stats\n",
        "\n",
        "\n",
        "class PrefixTreeNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.doc_ids = set()\n",
        "\n",
        "\n",
        "class Prompt:\n",
        "    def __init__(self, before, after):\n",
        "        self.before = before\n",
        "        self.after = after\n",
        "\n",
        "    def format(self, text):\n",
        "        return self.before + text + self.after\n",
        "\n",
        "\n",
        "class SequencePrefixTreeIndexStore(IndexStoreTemplate):\n",
        "    def __init__(\n",
        "        self,\n",
        "        transformer,\n",
        "        id_len,\n",
        "        universe,\n",
        "        doc_prompt: Prompt,\n",
        "        query_prompt: Prompt,\n",
        "        verbose=False,\n",
        "        initial_capacity=1000,\n",
        "        insertion_depth=3,\n",
        "        mode=\"document_search\",\n",
        "    ):\n",
        "        super().__init__(initial_capacity)\n",
        "\n",
        "        assert mode in [\"duplicate_detection\", \"document_search\"]\n",
        "        self.mode = mode\n",
        "\n",
        "        self.doc_prompt = doc_prompt\n",
        "        self.query_prompt = query_prompt\n",
        "\n",
        "        # Model for generating indices for inserted documens\n",
        "        self.transformer: GenXTransformer = transformer\n",
        "        self.id_len = id_len\n",
        "        self.universe = set(universe)\n",
        "\n",
        "        # Verbose\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Prefix tree\n",
        "        self.root = PrefixTreeNode()\n",
        "        self.insertion_depth = insertion_depth\n",
        "\n",
        "    def save_state(self, filepath):\n",
        "        import joblib\n",
        "\n",
        "        # Create a state dictionary with all attributes except transformer\n",
        "        state = {\n",
        "            # Capacity and sizing\n",
        "            \"_initial_capacity\": self._initial_capacity,\n",
        "            \"capacity\": self.capacity,\n",
        "            \"next_id\": self.next_id,\n",
        "            \"size\": self.size,\n",
        "            # Data storage\n",
        "            \"_ids\": self._ids,\n",
        "            \"_data_store\": self._data_store,\n",
        "            \"_beams_store\": self._beams_store,\n",
        "            \"_original_keys\": self._original_keys,\n",
        "            \"_original_keys_transpose\": self._original_keys_transpose,\n",
        "            # Configuration\n",
        "            \"mode\": self.mode,\n",
        "            \"doc_prompt\": self.doc_prompt,\n",
        "            \"query_prompt\": self.query_prompt,\n",
        "            \"id_len\": self.id_len,\n",
        "            \"universe\": self.universe,\n",
        "            \"verbose\": self.verbose,\n",
        "            \"insertion_depth\": self.insertion_depth,\n",
        "            # Prefix tree\n",
        "            \"root\": self.root,\n",
        "        }\n",
        "\n",
        "        # Save using joblib\n",
        "        joblib.dump(state, filepath)\n",
        "        print(f\"State saved to {filepath}\")\n",
        "\n",
        "    def load_state(self, filepath):\n",
        "        import joblib\n",
        "\n",
        "        # Load the state\n",
        "        state = joblib.load(filepath)\n",
        "\n",
        "        # Restore all attributes\n",
        "        self._initial_capacity = state[\"_initial_capacity\"]\n",
        "        self.capacity = state[\"capacity\"]\n",
        "        self.next_id = state[\"next_id\"]\n",
        "        self.size = state[\"size\"]\n",
        "        self._ids = state[\"_ids\"]\n",
        "        self._data_store = state[\"_data_store\"]\n",
        "        self._beams_store = state[\"_beams_store\"]\n",
        "        self._original_keys = state[\"_original_keys\"]\n",
        "        self._original_keys_transpose = state[\"_original_keys_transpose\"]\n",
        "        self.mode = state[\"mode\"]\n",
        "        self.doc_prompt = state[\"doc_prompt\"]\n",
        "        self.query_prompt = state[\"query_prompt\"]\n",
        "        self.id_len = state[\"id_len\"]\n",
        "        self.universe = state[\"universe\"]\n",
        "        self.verbose = state[\"verbose\"]\n",
        "        self.insertion_depth = state[\"insertion_depth\"]\n",
        "        self.root = state[\"root\"]\n",
        "\n",
        "        print(f\"State loaded from {filepath}\")\n",
        "\n",
        "    def print_data(self):\n",
        "        for id, val in self._data_store.items():\n",
        "            print(val.get_text(), val.get_metadata())\n",
        "            print(self._beams_store[id])\n",
        "            print()\n",
        "\n",
        "    def print_beams_store(self):\n",
        "        for id, val in self._beams_store.items():\n",
        "            print(\n",
        "                id,\n",
        "                val,\n",
        "                self.transformer.doc_tokenizer.batch_decode(\n",
        "                    val, skip_special_tokens=False\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def set_verbose_for_all(self, verbose):\n",
        "        self.verbose = verbose\n",
        "        if hasattr(self.transformer, \"verbose\"):\n",
        "            self.transformer.verbose = verbose\n",
        "\n",
        "    def reset_id_len(self, id_len):\n",
        "        self.id_len = id_len\n",
        "        self.transformer.update_num_next_tokens(max_new_tokens=id_len)\n",
        "\n",
        "    def clear_store(self):\n",
        "        self.root = PrefixTreeNode()\n",
        "\n",
        "        super()._clear_store()\n",
        "        if self.verbose:\n",
        "            print(f\"Store cleared, current capacity: {self.capacity}\")\n",
        "\n",
        "    def _insert_document(self, texts: list[Document], prompt_template):\n",
        "        if not isinstance(texts, list):\n",
        "            texts = [texts]\n",
        "\n",
        "        self._resize_if_needed(len(texts))\n",
        "\n",
        "        doc_ids = []\n",
        "        template_texts = []\n",
        "        for text in texts:\n",
        "            doc_id = self.next_id\n",
        "            doc_ids.append(doc_id)\n",
        "            # Update index in data store\n",
        "            self.next_id += 1\n",
        "            self.size += 1\n",
        "\n",
        "            # Save text in data store\n",
        "            self._ids[self.size - 1] = doc_id\n",
        "            self._data_store[doc_id] = text\n",
        "            self._original_keys[doc_id] = text.get_metadata()[\"doc_id\"]\n",
        "            self._original_keys_transpose[text.get_metadata()[\"doc_id\"]] = doc_id\n",
        "\n",
        "            template_text = prompt_template(text.get_text())\n",
        "            template_texts.append(template_text)\n",
        "\n",
        "        # Generate beams of sequences\n",
        "        # [batch_size, num_return_sequences, sequence_length]\n",
        "        lst_of_sequences = self.transformer.index_doc(template_texts)\n",
        "        print(lst_of_sequences) if self.verbose else None\n",
        "        self._insert_sequences_into_tree(lst_of_sequences, doc_ids)\n",
        "\n",
        "    def _insert_sequences_into_tree(\n",
        "        self, lst_of_sequences: list[list[list[int]]], doc_ids: list[int]\n",
        "    ):\n",
        "        for sequences, doc_id in zip(lst_of_sequences, doc_ids):\n",
        "            # Store sequences of a doc in to database\n",
        "            self._beams_store[doc_id] = sequences\n",
        "\n",
        "            for seq in sequences:\n",
        "                print(f\"Tokens: {seq}\") if self.verbose else None\n",
        "                if len(seq) != self.id_len or not all(x in self.universe for x in seq):\n",
        "                    continue  # Skip invalid sequences\n",
        "\n",
        "                self._traverse_and_insert(seq, doc_id)\n",
        "\n",
        "    def _traverse_and_insert(self, seq, doc_id):\n",
        "        node = self.root\n",
        "        depth = 0\n",
        "\n",
        "        for idx in seq:\n",
        "            if idx not in node.children:\n",
        "                node.children[idx] = PrefixTreeNode()\n",
        "            node = node.children[idx]\n",
        "            depth += 1\n",
        "            if depth >= self.insertion_depth:\n",
        "                node.doc_ids.add(doc_id)\n",
        "                if self.verbose:\n",
        "                    print(f\"Inserted doc {doc_id} at depth {depth} of prefix tree\")\n",
        "\n",
        "    def insert(self, texts: list[Document]):\n",
        "        _texts = []\n",
        "        for text in texts:\n",
        "            doc_id = text.get_metadata()[\"doc_id\"]\n",
        "            if doc_id not in self._original_keys.values():\n",
        "                _texts.append(text)\n",
        "        if len(_texts) > 0:\n",
        "            print(f\"Inserting '{_texts}'\") if self.verbose else None\n",
        "            self._insert_document(_texts, self.doc_prompt.format)\n",
        "\n",
        "    def _query_with_prompt(\n",
        "        self,\n",
        "        query_texts: list[Document],\n",
        "        prompt_template,\n",
        "    ) -> list[list[dict]]:\n",
        "        if not query_texts:\n",
        "            return []\n",
        "\n",
        "        lst_of_result_ids = []\n",
        "        template_texts = []\n",
        "        for query_text in query_texts:\n",
        "            template_text = prompt_template(query_text.get_text())\n",
        "            print(template_text) if self.verbose else None\n",
        "            template_texts.append(template_text)\n",
        "\n",
        "        # [batch_size, num_return_sequences, sequence_length]\n",
        "        lst_of_sequences = self.transformer.index_query(template_texts)\n",
        "        for sequences in lst_of_sequences:\n",
        "            result_ids = []\n",
        "            for seq in sequences:\n",
        "                print(f\"Tokens: {seq}\") if self.verbose else None\n",
        "                if len(seq) != self.id_len or not all(x in self.universe for x in seq):\n",
        "                    continue\n",
        "\n",
        "                result = self._traverse_tree_for_query(seq)\n",
        "                if result:\n",
        "                    result[\"index_ids\"] = seq\n",
        "                    result[\"index_txt\"] = self.transformer.doc_tokenizer.batch_decode(\n",
        "                        seq, skip_special_tokens=False\n",
        "                    )\n",
        "                    result_ids.append(result)\n",
        "            print(\"Found results: \", result_ids) if self.verbose else None\n",
        "            lst_of_result_ids.append(result_ids)\n",
        "\n",
        "        return lst_of_result_ids\n",
        "\n",
        "    def _traverse_tree_for_query(self, seq):\n",
        "        node: PrefixTreeNode = self.root\n",
        "        found = True\n",
        "        depth = 0\n",
        "\n",
        "        for idx in seq:\n",
        "            if (idx not in node.children) and (depth < self.insertion_depth):\n",
        "                found = False\n",
        "                break\n",
        "            if (idx not in node.children) and (depth >= self.insertion_depth):\n",
        "                break\n",
        "            node = node.children[idx]\n",
        "            depth += 1\n",
        "\n",
        "        if found:\n",
        "            return {\"depth\": depth, \"doc_ids\": node.doc_ids}\n",
        "        return None\n",
        "\n",
        "    def query(self, query_texts: list[Document]):\n",
        "        print(f\"Querying for '{query_texts}'\") if self.verbose else None\n",
        "        return self._query_with_prompt(query_texts, self.query_prompt.format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################## Train ########################################\n",
        "\n",
        "\n",
        "def log_validation(\n",
        "    store,\n",
        "    scifact_dataloader,\n",
        "    accelerator,\n",
        "    epoch,\n",
        "    split: str,\n",
        "    global_step: int,\n",
        "    is_final_validation: bool = False,\n",
        "):\n",
        "    cited_doc_ids = []\n",
        "    results = []\n",
        "\n",
        "    for batch in scifact_dataloader:\n",
        "        # Use dict to automatically handle duplicates (keeps last occurrence)\n",
        "        unique_queries = {}\n",
        "        for query, doc_id in zip(batch[\"query\"], batch[\"doc_id\"]):\n",
        "            if query not in unique_queries:\n",
        "                unique_queries[query] = []\n",
        "            unique_queries[query].append(doc_id)\n",
        "\n",
        "        queries_to_be_queried = []\n",
        "        doc_ids_for_a_query = []\n",
        "        for query, doc_ids in unique_queries.items():\n",
        "            queries_to_be_queried.append(Document(query, {\"doc_ids\": doc_ids}))\n",
        "            doc_ids_for_a_query.append(doc_ids)\n",
        "\n",
        "        result = store.query(queries_to_be_queried)\n",
        "        results.extend(result)\n",
        "        cited_doc_ids.extend(doc_ids_for_a_query)\n",
        "\n",
        "    assert len(results) == len(cited_doc_ids)\n",
        "\n",
        "    total_predicted = 0\n",
        "    total_correctly_predicted = 0\n",
        "    total_gold = 0\n",
        "\n",
        "    for idx in range(len(results)):\n",
        "        result = results[idx]\n",
        "        cited_doc_id = cited_doc_ids[idx]\n",
        "\n",
        "        # Extract predicted document IDs\n",
        "        if len(result) > 0:\n",
        "            temp = []\n",
        "            for item in result:\n",
        "                temp.extend(item[\"doc_ids\"])\n",
        "            predicted = set(temp)\n",
        "        else:\n",
        "            predicted = set()\n",
        "\n",
        "        predicted = list(predicted)\n",
        "        for idx, pred in enumerate(predicted):\n",
        "            predicted[idx] = int(store._data_store[pred].get_metadata()[\"doc_id\"])\n",
        "\n",
        "        cited_doc_id = set(cited_doc_id)\n",
        "        predicted = set(predicted)\n",
        "\n",
        "        # Count metrics\n",
        "        total_predicted += len(predicted)\n",
        "        total_gold += len(cited_doc_id)\n",
        "\n",
        "        # Count correctly predicted abstracts (intersection)\n",
        "        correctly_predicted = predicted.intersection(set(cited_doc_id))\n",
        "        total_correctly_predicted += len(correctly_predicted)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    if total_predicted > 0:\n",
        "        precision = total_correctly_predicted / total_predicted\n",
        "    else:\n",
        "        precision = 0.0\n",
        "\n",
        "    if total_gold > 0:\n",
        "        recall = total_correctly_predicted / total_gold\n",
        "    else:\n",
        "        recall = 0.0\n",
        "\n",
        "    # Calculate F1 score\n",
        "    if precision + recall > 0:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    else:\n",
        "        f1_score = 0.0\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        \"epoch\": epoch,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1_score,\n",
        "    }\n",
        "\n",
        "    # Log results\n",
        "    flag = split\n",
        "    if is_final_validation:\n",
        "        flag = f\"final-{split}\"\n",
        "    for tracker in accelerator.trackers:\n",
        "        if tracker.name == \"wandb\":\n",
        "            tracker.log(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    f\"{flag}-precision\": precision,\n",
        "                    f\"{flag}-recall\": recall,\n",
        "                    f\"{flag}-f1\": f1_score,\n",
        "                },\n",
        "                step=global_step,\n",
        "            )\n",
        "    return metrics\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Arguments:\n",
        "    # Model\n",
        "    query_model_alias: str = field(default=\"llama-3.2-1b-instruct\")\n",
        "    doc_model_alias: str = field(default=\"llama-3.2-1b-instruct\")\n",
        "    query_model_max_length: int = field(default=128)\n",
        "    doc_model_max_length: int = field(default=2048)\n",
        "    torch_dtype: str = field(default=\"bfloat16\")\n",
        "\n",
        "    # Prompts\n",
        "    doc_prompt_before: str = field(default=\"\")\n",
        "    doc_prompt_after: str = field(\n",
        "        default=\"Give me keywords. Do NOT use bullet points, numbered lists, or line breaks. Keep the output as one continuous block of text.\"\n",
        "    )\n",
        "    query_prompt_before: str = field(default=\"\")\n",
        "    query_prompt_after: str = field(\n",
        "        default=\"Give me potential keywords of related articles.\"\n",
        "    )\n",
        "    duplicate_prompt_before: str = field(\n",
        "        default=\"Given the text first remove all the punctuations and stop words. Then shuffle the sentences. Generate some unique related phrases that does not have synonyms. \"\n",
        "    )\n",
        "    duplicate_prompt_after: str = field(default=\" IGNORE ME. Phrases: \")\n",
        "\n",
        "    # Data\n",
        "    data_path: str = field(\n",
        "        default=\"/data/users/zy45/fbsource/fbcode/gen_ai/web_search/fbsearch/scripts/data/scifact/\"\n",
        "    )\n",
        "    train_queries_filename: str = field(default=\"claims_train_subset.jsonl\")\n",
        "    dev_queries_filename: str = field(default=\"claims_dev_subset.jsonl\")\n",
        "    syn_queries_filename: str = field(default=\"claims_syn_all_dedup.jsonl\")\n",
        "    corpus_filename: str = field(default=\"corpus_subset.jsonl\")\n",
        "    store_state_filename: str = field(default=\"store.joblib\")\n",
        "    output_dir: str = field(\n",
        "        default=\"/data/users/zy45/fbsource/fbcode/gen_ai/web_search/fbsearch/scripts/runs/scifact/train-on-real-sub-3\"\n",
        "    )\n",
        "\n",
        "    # Loading\n",
        "    load_store_state: bool = field(default=False)\n",
        "\n",
        "    # Logging\n",
        "    logging_dir: str = field(default=\"logs\")\n",
        "    tracker_name: str = field(default=\"scifact\")\n",
        "\n",
        "    # Device\n",
        "    device: str = field(default=\"cuda\")\n",
        "\n",
        "    # Seed\n",
        "    seed: int = field(default=0)\n",
        "\n",
        "    # Accelerator\n",
        "    mixed_precision: str = field(default=\"bf16\")\n",
        "    report_to: str = field(default=\"wandb\")\n",
        "    do_report: bool = field(default=False)\n",
        "\n",
        "    # Indexing\n",
        "    num_beams: int = field(default=1)\n",
        "    num_next_tokens: int = field(default=3)\n",
        "    insertion_depth: int = field(default=3)\n",
        "\n",
        "    # Training\n",
        "    per_device_train_batch_size: int = field(default=16)\n",
        "    lr_warmup_steps: int = field(default=100)\n",
        "    lr_scheduler: str = field(default=\"cosine\")\n",
        "    gradient_accumulation_steps: int = field(default=1)\n",
        "    num_train_epochs: int = field(default=20)\n",
        "    learning_rate: float = field(default=5e-5)\n",
        "    adam_beta1: float = field(default=0.9)\n",
        "    adam_beta2: float = field(default=0.999)\n",
        "    adam_epsilon: float = field(default=1e-8)\n",
        "    adam_weight_decay: float = field(default=0.0)\n",
        "    max_grad_norm: float = field(default=1.0)\n",
        "    dataloader_num_workers: int = field(default=4)\n",
        "    validation_epochs: int = field(default=20)\n",
        "    train_on_syn_data: bool = field(default=False)\n",
        "\n",
        "    # Resume checkpoint\n",
        "    resume_from_checkpoint: str = field(default=None)\n",
        "    checkpointing_epochs: int = field(default=99999)\n",
        "\n",
        "\n",
        "logger = get_logger(__name__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: bf16\n",
            "\n",
            "09/19/2025 11:17:01 - INFO - __main__ - Inserting corpus into data store...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DDP training\n",
            "Created a repo /data/users/zy45/fbsource/fbcode/gen_ai/web_search/fbsearch/scripts/runs/scifact/train-on-real-sub-3\n",
            "{'max_new_tokens': 3, 'do_sample': False, 'num_beams': 1, 'num_return_sequences': 1, 'eos_token_id': None}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - ***** Running training *****\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - Num examples = 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - Num batches each epoch = 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - Num epochs = 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - Instantaneous batch size per device = 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - Gradient accumulation steps = 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - Number of update steps per epoch = 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - Total optimization steps = 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - Checkpointing epochs = 99999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 11:17:01 - INFO - __main__ - Validation epochs = 20\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State saved to /data/users/zy45/fbsource/fbcode/gen_ai/web_search/fbsearch/scripts/runs/scifact/train-on-real-sub-3/store.joblib\n",
            "0 [[315, 3823, 46397]] [' of human newborn']\n",
            "1 [[1065, 501, 5174]] ['ysplastic']\n",
            "2 [[4645, 41214, 11]] ['101 RNA,']\n",
            "3 [[42972, 638, 315]] ['ethylome of']\n"
          ]
        }
      ],
      "source": [
        "args = Arguments()\n",
        "\n",
        "if args:\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    print(\"DDP training\")\n",
        "\n",
        "    # Accelerator\n",
        "    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
        "    accelerator_project_config = ProjectConfiguration(\n",
        "        project_dir=args.output_dir, logging_dir=logging_dir\n",
        "    )\n",
        "    kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        mixed_precision=args.mixed_precision,\n",
        "        log_with=args.report_to,\n",
        "        project_config=accelerator_project_config,\n",
        "        kwargs_handlers=[kwargs],\n",
        "    )\n",
        "\n",
        "    # Make one log on every process with the configuration for debugging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    logger.info(accelerator.state, main_process_only=False)\n",
        "    if accelerator.is_local_main_process:\n",
        "        transformers_utils_logging.set_verbosity_warning()\n",
        "    else:\n",
        "        transformers_utils_logging.set_verbosity_error()\n",
        "\n",
        "    # Seed\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if accelerator.is_main_process:\n",
        "        if args.output_dir is not None:\n",
        "            os.makedirs(args.output_dir, exist_ok=True)\n",
        "            print(f\"Created a repo {args.output_dir}\")\n",
        "\n",
        "    # Data\n",
        "    data_path = args.data_path\n",
        "    train_queries_path = os.path.join(data_path, args.train_queries_filename)\n",
        "    syn_queries_path = os.path.join(data_path, args.syn_queries_filename)\n",
        "    dev_queries_path = os.path.join(data_path, args.dev_queries_filename)\n",
        "    corpus_path = os.path.join(data_path, args.corpus_filename)\n",
        "\n",
        "    per_device_train_batch_size = args.per_device_train_batch_size\n",
        "\n",
        "    # Prompt templates\n",
        "    corpus_prompt_template = Prompt(\n",
        "        before=args.doc_prompt_before,\n",
        "        after=args.doc_prompt_after,\n",
        "    )\n",
        "    query_prompt_template = Prompt(\n",
        "        before=args.query_prompt_before,\n",
        "        after=args.query_prompt_after,\n",
        "    )\n",
        "\n",
        "    # Corpus\n",
        "    corpus_dataset = SciFactCorpusDataset(\n",
        "        corpus_path,\n",
        "    )\n",
        "    corpus_dataloader = DataLoader(\n",
        "        corpus_dataset,\n",
        "        batch_size=per_device_train_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=args.dataloader_num_workers,\n",
        "    )\n",
        "    corpus_dict = corpus_dataset.get_corpus_dict()\n",
        "\n",
        "    # Queries\n",
        "    real_train_dataloader = get_scifact_query_dataloader(\n",
        "        queries_path=train_queries_path,\n",
        "        corpus_dict=corpus_dict,\n",
        "        batch_size=per_device_train_batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args.dataloader_num_workers,\n",
        "    )\n",
        "    if args.train_on_syn_data:\n",
        "        train_dataloader = get_scifact_query_dataloader(\n",
        "            queries_path=[syn_queries_path, train_queries_path],\n",
        "            corpus_dict=corpus_dict,\n",
        "            batch_size=per_device_train_batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=args.dataloader_num_workers,\n",
        "        )\n",
        "    else:\n",
        "        train_dataloader = real_train_dataloader\n",
        "\n",
        "    dev_dataloader = get_scifact_query_dataloader(\n",
        "        queries_path=dev_queries_path,\n",
        "        corpus_dict=corpus_dict,\n",
        "        batch_size=per_device_train_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=args.dataloader_num_workers,\n",
        "    )\n",
        "\n",
        "    # Data type\n",
        "    torch_dtype = getattr(torch, args.torch_dtype)\n",
        "\n",
        "    # Model\n",
        "    if \"query_model\" not in locals():\n",
        "        print(\"Loading models...\")\n",
        "        query_model_name_or_path = TRANSFORMERS_PATH_MAP[args.query_model_alias]\n",
        "        doc_model_name_or_path = TRANSFORMERS_PATH_MAP[args.doc_model_alias]\n",
        "        if \"query_model_copy\" not in locals():\n",
        "            query_model_copy, query_tokenizer = get_model_and_tokenizer(\n",
        "                query_model_name_or_path,\n",
        "                model_max_length=args.query_model_max_length,\n",
        "                torch_dtype=torch_dtype,\n",
        "            )\n",
        "        if \"doc_model\" not in locals():\n",
        "            doc_model, doc_tokenizer = get_model_and_tokenizer(\n",
        "                doc_model_name_or_path,\n",
        "                model_max_length=args.doc_model_max_length,\n",
        "                torch_dtype=torch_dtype,\n",
        "            )\n",
        "            doc_model.to(accelerator.device, dtype=torch_dtype)\n",
        "\n",
        "        query_model = copy.deepcopy(query_model_copy)\n",
        "        query_model.to(accelerator.device, dtype=torch_dtype)\n",
        "\n",
        "    # Only train the query model\n",
        "    query_model.train()\n",
        "    doc_model.eval()\n",
        "    doc_model.requires_grad_(False)\n",
        "\n",
        "    # Optimizer and learning rate scheduler\n",
        "    num_warmup_steps_for_scheduler = args.lr_warmup_steps * accelerator.num_processes\n",
        "    len_train_dataloader_after_sharding = math.ceil(\n",
        "        len(train_dataloader) / accelerator.num_processes\n",
        "    )\n",
        "    num_update_steps_per_epoch = math.ceil(\n",
        "        len_train_dataloader_after_sharding / args.gradient_accumulation_steps\n",
        "    )\n",
        "    num_training_steps_for_scheduler = (\n",
        "        args.num_train_epochs * accelerator.num_processes * num_update_steps_per_epoch\n",
        "    )\n",
        "    params_to_optimize = [\n",
        "        {\n",
        "            \"params\": list(filter(lambda p: p.requires_grad, query_model.parameters())),\n",
        "            \"lr\": args.learning_rate,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        params_to_optimize,\n",
        "        betas=(args.adam_beta1, args.adam_beta2),\n",
        "        weight_decay=args.adam_weight_decay,\n",
        "        eps=args.adam_epsilon,\n",
        "    )\n",
        "    lr_scheduler = get_scheduler(\n",
        "        args.lr_scheduler,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=num_warmup_steps_for_scheduler,\n",
        "        num_training_steps=num_training_steps_for_scheduler,\n",
        "    )\n",
        "\n",
        "    # The size of the training dataloader may have changed due to accelerator.prepare, so we need to recalculate our total training steps\n",
        "    num_update_steps_per_epoch = math.ceil(\n",
        "        len(train_dataloader) / args.gradient_accumulation_steps\n",
        "    )\n",
        "    max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "    if num_training_steps_for_scheduler != max_train_steps:\n",
        "        logger.warning(\n",
        "            f\"The length of the 'train_dataloader' after 'accelerator.prepare' ({len(train_dataloader)}) does not match \"\n",
        "            f\"the expected length ({len_train_dataloader_after_sharding}) when the learning rate scheduler was created. \"\n",
        "            f\"This inconsistency may result in the learning rate scheduler not functioning properly.\"\n",
        "        )\n",
        "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    genx_transformer = GenXTransformer(\n",
        "        query_model=query_model,\n",
        "        doc_model=doc_model,\n",
        "        query_tokenizer=query_tokenizer,\n",
        "        doc_tokenizer=doc_tokenizer,\n",
        "        num_beams=args.num_beams,\n",
        "        num_next_tokens=args.num_next_tokens,\n",
        "    )\n",
        "    print(genx_transformer.genx_gen_kwargs)\n",
        "\n",
        "    # Prepare everything with our accelerator\n",
        "    (\n",
        "        query_model,\n",
        "        doc_model,\n",
        "        optimizer,\n",
        "        corpus_dataloader,\n",
        "        train_dataloader,\n",
        "        dev_dataloader,\n",
        "        lr_scheduler,\n",
        "    ) = accelerator.prepare(\n",
        "        query_model,\n",
        "        doc_model,\n",
        "        optimizer,\n",
        "        corpus_dataloader,\n",
        "        train_dataloader,\n",
        "        dev_dataloader,\n",
        "        lr_scheduler,\n",
        "    )\n",
        "\n",
        "    # Report\n",
        "    if accelerator.is_main_process and args.do_report:\n",
        "        tracker_name = args.tracker_name\n",
        "        accelerator.init_trackers(tracker_name, config=vars(args))\n",
        "\n",
        "    store = SequencePrefixTreeIndexStore(\n",
        "        genx_transformer,\n",
        "        id_len=args.num_next_tokens,\n",
        "        universe=set(range(genx_transformer.doc_tokenizer.vocab_size)),\n",
        "        doc_prompt=corpus_prompt_template,\n",
        "        query_prompt=query_prompt_template,\n",
        "        mode=\"document_search\",\n",
        "        insertion_depth=args.insertion_depth,\n",
        "    )\n",
        "    store.clear_store()\n",
        "    store.set_verbose_for_all(False)\n",
        "\n",
        "    store_state_path = os.path.join(args.output_dir, args.store_state_filename)\n",
        "    if args.load_store_state and os.path.exists(store_state_path):\n",
        "        logger.info(\"Loading state...\")\n",
        "        store.load_state(store_state_path)\n",
        "        store.print_beams_store()\n",
        "        print()\n",
        "        store.plot_list_frequencies(\n",
        "            store._beams_store,\n",
        "            figsize=(8, 6),\n",
        "            save_path=os.path.join(args.output_dir, \"freq.pdf\"),\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Inserting corpus into data store...\")\n",
        "        for batch in corpus_dataloader:\n",
        "            doc_ids = batch[\"doc_id\"]\n",
        "            texts = batch[\"text\"]\n",
        "            to_be_inserted = []\n",
        "            for doc_id, text in zip(doc_ids, texts):\n",
        "                doc_id = doc_id.cpu().item()\n",
        "                to_be_inserted.append(Document(text, {\"doc_id\": doc_id}))\n",
        "            store.insert(to_be_inserted)\n",
        "        store.save_state(store_state_path)\n",
        "        store.print_beams_store()\n",
        "\n",
        "    # Log some info about our training\n",
        "    total_batch_size = (\n",
        "        per_device_train_batch_size\n",
        "        * accelerator.num_processes\n",
        "        * args.gradient_accumulation_steps\n",
        "    )\n",
        "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"Num examples = {len(train_dataloader.dataset)}\")\n",
        "    logger.info(f\"Num batches each epoch = {len(train_dataloader)}\")\n",
        "    logger.info(f\"Num epochs = {num_train_epochs}\")\n",
        "    logger.info(f\"Instantaneous batch size per device = {per_device_train_batch_size}\")\n",
        "    logger.info(\n",
        "        f\"Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
        "    )\n",
        "    logger.info(f\"Gradient accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "    logger.info(f\"Number of update steps per epoch = {num_update_steps_per_epoch}\")\n",
        "    logger.info(f\"Total optimization steps = {max_train_steps}\")\n",
        "    logger.info(f\"Checkpointing epochs = {args.checkpointing_epochs}\")\n",
        "    logger.info(f\"Validation epochs = {args.validation_epochs}\")\n",
        "    checkpointing_steps = args.checkpointing_epochs * num_update_steps_per_epoch\n",
        "    global_step = 0\n",
        "    first_epoch = 0\n",
        "\n",
        "    best_dev_f1 = 0.0\n",
        "    best_train_metrics = {}\n",
        "    best_dev_metrics = {}\n",
        "\n",
        "    # Potentially load in the weights and states from a previous save\n",
        "    if args.resume_from_checkpoint:\n",
        "        if args.resume_from_checkpoint != \"latest\":\n",
        "            path = os.path.basename(args.resume_from_checkpoint)\n",
        "        else:\n",
        "            # Get the mos recent checkpoint\n",
        "            dirs = os.listdir(args.output_dir)\n",
        "            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
        "            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
        "            path = dirs[-1] if len(dirs) > 0 else None\n",
        "\n",
        "        if path is None:\n",
        "            accelerator.print(\n",
        "                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
        "            )\n",
        "            initial_global_step = 0\n",
        "        else:\n",
        "            accelerator.print(f\"Resuming from checkpoint {path}\")\n",
        "            accelerator.load_state(os.path.join(args.output_dir, path))\n",
        "            global_step = int(path.split(\"-\")[1])\n",
        "\n",
        "            initial_global_step = global_step\n",
        "            first_epoch = global_step // num_update_steps_per_epoch\n",
        "    else:\n",
        "        initial_global_step = 0\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        range(0, max_train_steps),\n",
        "        initial=initial_global_step,\n",
        "        desc=\"Steps\",\n",
        "        # Only show the progress bar once on each machine.\n",
        "        # disable=not accelerator.is_local_main_process,\n",
        "        disable=True,\n",
        "    )\n",
        "    epoch = first_epoch\n",
        "    for epoch in range(first_epoch, num_train_epochs):\n",
        "        query_model.train()\n",
        "        for _, batch in enumerate(train_dataloader):\n",
        "            models_to_accumulate = [query_model]\n",
        "            with accelerator.accumulate(models_to_accumulate):\n",
        "                doc_ids = batch[\"doc_id\"]\n",
        "                queries = batch[\"query\"]\n",
        "                docs = batch[\"text\"]\n",
        "                docs = [corpus_prompt_template.format(text) for text in docs]\n",
        "                queries = [query_prompt_template.format(query) for query in queries]\n",
        "\n",
        "                beams_for_docs = []\n",
        "                all_processed = True\n",
        "                for doc_id in doc_ids:\n",
        "                    if doc_id not in store._original_keys.values():\n",
        "                        all_processed = False\n",
        "                        break\n",
        "                if all_processed:\n",
        "                    for doc_id in doc_ids:\n",
        "                        id = store._original_keys_transpose[doc_id]\n",
        "                        beams_for_docs.append(store._beams_store[id])\n",
        "                    loss = genx_transformer(queries, beams_for_docs)\n",
        "                else:\n",
        "                    loss = genx_transformer(queries, docs)\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                if accelerator.sync_gradients:\n",
        "                    params_to_clip = itertools.chain(query_model.parameters())\n",
        "                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "\n",
        "                if accelerator.is_main_process:\n",
        "                    if global_step % checkpointing_steps == 0:\n",
        "                        save_path = os.path.join(\n",
        "                            args.output_dir, f\"checkpoint-{global_step}\"\n",
        "                        )\n",
        "                        accelerator.save_state(save_path)\n",
        "                        logger.info(f\"Saved state to {save_path}\")\n",
        "\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "\n",
        "            if global_step >= max_train_steps:\n",
        "                break\n",
        "\n",
        "        # Validate!\n",
        "        if accelerator.is_main_process:\n",
        "            if epoch % args.validation_epochs == 0:\n",
        "                train_metrics = log_validation(\n",
        "                    store,\n",
        "                    real_train_dataloader,\n",
        "                    accelerator,\n",
        "                    epoch,\n",
        "                    split=\"train\",\n",
        "                    global_step=global_step,\n",
        "                    is_final_validation=False,\n",
        "                )\n",
        "                dev_metrics = log_validation(\n",
        "                    store,\n",
        "                    dev_dataloader,\n",
        "                    accelerator,\n",
        "                    epoch,\n",
        "                    split=\"dev\",\n",
        "                    global_step=global_step,\n",
        "                    is_final_validation=False,\n",
        "                )\n",
        "                dev_f1 = dev_metrics[\"f1\"]\n",
        "                if dev_f1 > best_dev_f1:\n",
        "                    best_dev_f1 = dev_f1\n",
        "                    best_dev_metrics = dev_metrics\n",
        "                    best_train_metrics = train_metrics\n",
        "\n",
        "    # Evaluate!\n",
        "    accelerator.wait_for_everyone()\n",
        "    if accelerator.is_main_process:\n",
        "        train_metrics = log_validation(\n",
        "            store,\n",
        "            real_train_dataloader,\n",
        "            accelerator,\n",
        "            epoch,\n",
        "            split=\"train\",\n",
        "            global_step=global_step,\n",
        "            is_final_validation=True,\n",
        "        )\n",
        "        dev_metrics = log_validation(\n",
        "            store,\n",
        "            dev_dataloader,\n",
        "            accelerator,\n",
        "            epoch,\n",
        "            split=\"dev\",\n",
        "            global_step=global_step,\n",
        "            is_final_validation=True,\n",
        "        )\n",
        "        dev_f1 = dev_metrics[\"f1\"]\n",
        "        if dev_f1 > best_dev_f1:\n",
        "            best_dev_f1 = dev_f1\n",
        "            best_dev_metrics = dev_metrics\n",
        "            best_train_metrics = train_metrics\n",
        "\n",
        "    # Finish\n",
        "    accelerator.end_training()\n",
        "    best_metrics = {\n",
        "        \"train\": best_train_metrics,\n",
        "        \"dev\": best_dev_metrics,\n",
        "    }\n",
        "    with open(os.path.join(args.output_dir, \"metrics.json\"), \"w\") as f:\n",
        "        json.dump(best_metrics, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Querying for '[<__main__.Document object at 0x7f7e3403f320>, <__main__.Document object at 0x7f7d701abda0>, <__main__.Document object at 0x7f7d701a9d00>, <__main__.Document object at 0x7f7d701abdd0>]'\n",
            "1-1% of colorectal cancer patients are diagnosed with regional or distant metastases.Give me potential keywords of related articles.\n",
            "1 in 5 million in UK have abnormal PrP positivity.Give me potential keywords of related articles.\n",
            "10% of sudden infant death syndrome (SIDS) deaths happen in newborns aged less than 6 months.Give me potential keywords of related articles.\n",
            "0-dimensional biomaterials lack inductive properties.Give me potential keywords of related articles.\n",
            "Decoded tokens: [' and the final', ' and a suggested', 'ethylome of', ' return 500']\n",
            "Token IDs: [[[323, 279, 1620]], [[323, 264, 12090]], [[42972, 638, 315]], [[471, 220, 2636]]]\n",
            "Tokens: [323, 279, 1620]\n",
            "Found results:  []\n",
            "Tokens: [323, 264, 12090]\n",
            "Found results:  []\n",
            "Tokens: [42972, 638, 315]\n",
            "Found results:  [{'depth': 3, 'doc_ids': {3}, 'index_ids': [42972, 638, 315], 'index_txt': ['ethyl', 'ome', ' of']}]\n",
            "Tokens: [471, 220, 2636]\n",
            "Found results:  []\n"
          ]
        }
      ],
      "source": [
        "store.set_verbose_for_all(True)\n",
        "train_metrics = log_validation(\n",
        "    store,\n",
        "    real_train_dataloader,\n",
        "    accelerator,\n",
        "    epoch,\n",
        "    split=\"train\",\n",
        "    global_step=global_step,\n",
        "    is_final_validation=True,\n",
        ")\n",
        "store.set_verbose_for_all(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "163bf236-5e43-41d4-bfe9-d3fb65e76208",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "fbs (conda)",
      "language": "python",
      "name": "conda_fbs"
    },
    "language_info": {
      "name": "plaintext"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
