{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import transformers.utils.logging as transformers_utils_logging\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import (\n",
        "    DistributedDataParallelKwargs,\n",
        "    ProjectConfiguration,\n",
        "    set_seed,\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    get_scheduler,\n",
        "    HfArgumentParser,\n",
        "    PreTrainedTokenizer,\n",
        ")\n",
        "\n",
        "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "\n",
        "TRANSFORMERS_PATH_MAP = {\n",
        "    \"llama-3.2-1b-instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    \"llama-3.1-8b-instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################## Datset ########################################\n",
        "\n",
        "\n",
        "def row2text_template_scifact(row):\n",
        "    text = f\"Title: {row['title']}\\nAbstract: {' '.join(row['abstract'])}\\nStructured: {row['structured']}\\n\"\n",
        "    # Remove all newlines and strip extra spaces\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    return text\n",
        "\n",
        "\n",
        "class SciFactCorpusDataset(Dataset):\n",
        "    def __init__(self, corpus_path):\n",
        "        super().__init__()\n",
        "        corpus = pd.read_json(corpus_path, lines=True)\n",
        "        corpus[\"text\"] = corpus.apply(row2text_template_scifact, axis=1)\n",
        "\n",
        "        self.corpus = corpus\n",
        "        self.corpus_dict = corpus.set_index(\"doc_id\")[\"text\"].to_dict()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        doc_id = self.corpus[\"doc_id\"][i]\n",
        "        text = self.corpus_dict.get(doc_id)\n",
        "        return {\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": text,\n",
        "        }\n",
        "\n",
        "    def get_corpus_dict(self):\n",
        "        return self.corpus_dict\n",
        "\n",
        "\n",
        "def get_scifact_corpus_dataloader(\n",
        "    corpus_path,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "):\n",
        "    scifact_corpus_dataset = SciFactCorpusDataset(corpus_path)\n",
        "    dataloader = DataLoader(\n",
        "        scifact_corpus_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "class SciFactQueryDataset(Dataset):\n",
        "    def __init__(self, queries_path, corpus_dict):\n",
        "        super().__init__()\n",
        "        if isinstance(queries_path, str):\n",
        "            self.queries = pd.read_json(queries_path, lines=True)\n",
        "        elif isinstance(queries_path, list):\n",
        "            queries = []\n",
        "            for path in queries_path:\n",
        "                queries.append(pd.read_json(path, lines=True))\n",
        "            self.queries = pd.concat(queries, ignore_index=True)\n",
        "        else:\n",
        "            raise ValueError(\"queries_path must be a string or a list of strings\")\n",
        "\n",
        "        self.corpus_dict = corpus_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        doc_id_list = self.queries[\"cited_doc_ids\"][i]\n",
        "        query = self.queries[\"claim\"][i]\n",
        "\n",
        "        docs = []\n",
        "        for doc_id in doc_id_list:\n",
        "            docs.append(self.corpus_dict.get(doc_id))\n",
        "\n",
        "        n_docs = len(docs)\n",
        "\n",
        "        if n_docs == 0:\n",
        "            return {}\n",
        "        else:\n",
        "            queries = [query] * n_docs\n",
        "\n",
        "        return {\n",
        "            \"doc_id\": doc_id_list,\n",
        "            \"query\": queries,\n",
        "            \"text\": docs,\n",
        "        }\n",
        "\n",
        "\n",
        "def scifact_query_collate_fn(batch):\n",
        "    batch = [item for item in batch if item]\n",
        "    if not batch:\n",
        "        return {}\n",
        "    doc_ids = sum([item[\"doc_id\"] for item in batch], [])\n",
        "    queries = sum([item[\"query\"] for item in batch], [])\n",
        "    texts = sum([item[\"text\"] for item in batch], [])\n",
        "    return {\n",
        "        \"doc_id\": doc_ids,\n",
        "        \"query\": queries,\n",
        "        \"text\": texts,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_scifact_query_dataloader(\n",
        "    queries_path: str | list[str],\n",
        "    corpus_dict,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "):\n",
        "    scifact_query_dataset = SciFactQueryDataset(\n",
        "        queries_path,\n",
        "        corpus_dict,\n",
        "    )\n",
        "    dataloader = DataLoader(\n",
        "        scifact_query_dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=scifact_query_collate_fn,  # NOTE: This is for query dataset only\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################## Model ########################################\n",
        "\n",
        "\n",
        "def get_special_tokens_dict(tokenizer):\n",
        "    special_tokens_dict = {}\n",
        "    if tokenizer.pad_token is None:\n",
        "        special_tokens_dict[\"pad_token\"] = \"[PAD]\"\n",
        "    if tokenizer.eos_token is None:\n",
        "        special_tokens_dict[\"eos_token\"] = \"</s>\"\n",
        "    if tokenizer.bos_token is None:\n",
        "        special_tokens_dict[\"bos_token\"] = \"<s>\"\n",
        "    if tokenizer.unk_token is None:\n",
        "        special_tokens_dict[\"unk_token\"] = \"<unk>\"\n",
        "    return special_tokens_dict\n",
        "\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict,\n",
        "    tokenizer,\n",
        "    model,\n",
        "):\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
        "            dim=0, keepdim=True\n",
        "        )\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
        "            dim=0, keepdim=True\n",
        "        )\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "\n",
        "def _tokenize_fn(strings: list[str], tokenizer: PreTrainedTokenizer):\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
        "        for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_model_and_tokenizer(\n",
        "    model_name_or_path,\n",
        "    model_max_length=1024,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    padding_side=\"right\",\n",
        "):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        model_max_length=model_max_length,\n",
        "        padding_side=padding_side,\n",
        "        use_fast=False,\n",
        "    )\n",
        "    special_tokens_dict = get_special_tokens_dict(tokenizer)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch_dtype,\n",
        "    )\n",
        "    smart_tokenizer_and_embedding_resize(\n",
        "        special_tokens_dict=special_tokens_dict,\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "class GenXTransformer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        query_model,\n",
        "        doc_model,\n",
        "        train_tokenizer,\n",
        "        inference_tokenizer,\n",
        "        num_beams: int = 5,\n",
        "        num_next_tokens: int = 5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.query_model = query_model\n",
        "        self.doc_model = doc_model\n",
        "\n",
        "        self.train_tokenizer = train_tokenizer\n",
        "        self.inference_tokenizer = inference_tokenizer\n",
        "\n",
        "        self.num_beams = num_beams\n",
        "        self.num_next_tokens = num_next_tokens\n",
        "\n",
        "        self.verbose = False\n",
        "\n",
        "        self.config_genx_gen_kwargs(\n",
        "            num_beams=num_beams,\n",
        "            num_return_sequences=num_beams,\n",
        "            max_new_tokens=num_next_tokens,\n",
        "        )\n",
        "\n",
        "    def set_train_eval_mode(self, query_train: bool = True, doc_train: bool = False):\n",
        "        if query_train:\n",
        "            self.query_model.train()\n",
        "        else:\n",
        "            self.query_model.eval()\n",
        "        if doc_train:\n",
        "            self.doc_model.train()\n",
        "        else:\n",
        "            self.doc_model.eval()\n",
        "\n",
        "    def update_num_next_tokens(self, num_next_tokens: int):\n",
        "        self.num_next_tokens = num_next_tokens\n",
        "        self.genx_gen_kwargs[\"max_new_tokens\"] = num_next_tokens\n",
        "\n",
        "    def config_genx_gen_kwargs(self, **kwargs):\n",
        "        gen_kwargs = {\n",
        "            \"max_new_tokens\": kwargs.get(\"max_new_tokens\", 5),\n",
        "            \"do_sample\": False,\n",
        "            \"num_beams\": kwargs.get(\"num_beams\", 1),\n",
        "            \"num_return_sequences\": kwargs.get(\"num_return_sequences\", 1),\n",
        "            \"eos_token_id\": kwargs.get(\"eos_token_id\", None),\n",
        "        }\n",
        "        self.genx_gen_kwargs = gen_kwargs\n",
        "\n",
        "    def index_prompt(self, prompts, model, tokenizer, shift=0):\n",
        "\n",
        "        device = model.device\n",
        "\n",
        "        if isinstance(prompts, str):\n",
        "            prompts = [prompts]\n",
        "\n",
        "        batch = tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "        )\n",
        "\n",
        "        genx_gen_kwargs = self.genx_gen_kwargs.copy()\n",
        "        genx_gen_kwargs[\"max_new_tokens\"] += shift\n",
        "\n",
        "        bad_chars = [\"#\", \"\\n\", \".\", \"}\", \"{\", \"step\"] + list(\"0123456789\")\n",
        "        bad_words_ids = [\n",
        "            tokenizer.encode(char, add_special_tokens=False) for char in bad_chars\n",
        "        ]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            genx_gen_kwargs[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
        "            genx_gen_kwargs[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
        "            generated_tokens = model.generate(\n",
        "                **genx_gen_kwargs,\n",
        "                bad_words_ids=bad_words_ids,\n",
        "            )\n",
        "\n",
        "        input_len = len(batch[\"input_ids\"][0])\n",
        "        pred_next_tokens = generated_tokens[:, input_len + shift :]\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Decoded tokens:\",\n",
        "                tokenizer.batch_decode(pred_next_tokens, skip_special_tokens=False),\n",
        "            )\n",
        "\n",
        "        batch_size = len(prompts)\n",
        "        num_return_sequences = genx_gen_kwargs[\"num_return_sequences\"]\n",
        "\n",
        "        pred_next_tokens = pred_next_tokens.view(batch_size, num_return_sequences, -1)\n",
        "        pred_next_tokens = pred_next_tokens.cpu().tolist()\n",
        "\n",
        "        print(\"Token IDs:\", pred_next_tokens) if self.verbose else None\n",
        "        return pred_next_tokens\n",
        "\n",
        "    def index_query(self, prompts: list[str]):\n",
        "        return self.index_prompt(prompts, self.query_model, self.inference_tokenizer, 0)\n",
        "\n",
        "    def index_doc(self, prompts: list[str]):\n",
        "        return self.index_prompt(prompts, self.doc_model, self.inference_tokenizer, 3)\n",
        "\n",
        "    def get_sft_loss_txt(self, model, tokenizer, prompts: list[str]):\n",
        "        tokens = tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=tokenizer.model_max_length,\n",
        "        ).to(model.device)\n",
        "\n",
        "        input_ids = tokens[\"input_ids\"]\n",
        "        attention_mask = tokens[\"attention_mask\"]\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        keep = self.num_next_tokens\n",
        "        for i, mask in enumerate(attention_mask):\n",
        "            length = mask.sum().item()\n",
        "            cutoff = max(0, length - keep)\n",
        "            labels[i, :cutoff] = IGNORE_INDEX\n",
        "            labels[i, length:] = IGNORE_INDEX\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "        return outputs.loss\n",
        "\n",
        "    def __call__(self, queries: list[str], docs: list[str]):\n",
        "        # Now this is fine-tuning query model to generate next tokens of document\n",
        "        assert len(queries) == len(docs)\n",
        "\n",
        "        if isinstance(docs[0], str):\n",
        "            beams_for_docs: list[list[list[int]]] = self.index_doc(\n",
        "                docs,\n",
        "            )  # Shape is num_docs x num_beams x num_next_tokens\n",
        "        else:\n",
        "            beams_for_docs = docs\n",
        "\n",
        "        # Shape is num_docs x num_beams x (len(query) + num_next_tokens)\n",
        "        prompts_for_all_pairs: list[list[str]] = []\n",
        "        for doc_idx, beams in enumerate(beams_for_docs):\n",
        "            beams = self.inference_tokenizer.batch_decode(beams, skip_special_tokens=False)\n",
        "            prompts = []  # List of the same query and num_beams possible next sentences\n",
        "\n",
        "            query = queries[doc_idx]\n",
        "            num_beams = len(beams)\n",
        "            for beams_idx in range(num_beams):\n",
        "                prompt = query + beams[beams_idx]\n",
        "                prompts.append(prompt)\n",
        "            prompts_for_all_pairs.append(prompts)\n",
        "\n",
        "        # Have num_docs x num_beams sequences, each of a string of length (len(query) + num_next_tokens)\n",
        "        flats: list[str] = list(itertools.chain.from_iterable(prompts_for_all_pairs))\n",
        "        loss = self.get_sft_loss_txt(self.query_model, self.train_tokenizer, flats)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################## Store ########################################\n",
        "\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, text, metadata):\n",
        "        self._text = text\n",
        "        self._metadata = metadata\n",
        "\n",
        "    def get_text(self):\n",
        "        return self._text\n",
        "\n",
        "    def get_metadata(self):\n",
        "        return self._metadata\n",
        "\n",
        "\n",
        "class IndexStoreTemplate(ABC):\n",
        "    def __init__(self, initial_capacity=1000):\n",
        "        # Capacity and initial capacity\n",
        "        self._initial_capacity = initial_capacity\n",
        "        self.capacity = initial_capacity\n",
        "\n",
        "        # Data and global index of elements\n",
        "        self.next_id = 0\n",
        "        self.size = 0\n",
        "        self._ids = np.zeros(self.capacity, dtype=np.int64)\n",
        "        self._data_store = {}  # id to text\n",
        "        self._beams_store = {}  # id to beams\n",
        "        self._original_keys = {}  # id to original key in the dataset\n",
        "        self._original_keys_transpose = {}\n",
        "\n",
        "    def _resize_if_needed(self, additional_items=16):\n",
        "        if self.size + additional_items > self.capacity:\n",
        "            new_capacity = max(self.capacity * 2, self.size + additional_items)\n",
        "\n",
        "            # Resize ID array\n",
        "            new_ids = np.zeros(new_capacity, dtype=np.int64)\n",
        "            new_ids[: self.size] = self._ids[: self.size]\n",
        "            self._ids = new_ids\n",
        "\n",
        "            self.capacity = new_capacity\n",
        "\n",
        "    def _clear_store(self):\n",
        "        self.capacity = self._initial_capacity\n",
        "        self.next_id = 0\n",
        "        self.size = 0\n",
        "        self._ids = np.zeros(self._initial_capacity, dtype=np.int64)\n",
        "        self._data_store = {}\n",
        "        self._beams_store = {}\n",
        "        self._original_keys = {}\n",
        "        self._original_keys_transpose = {}\n",
        "\n",
        "    def retrieve(self, doc_id):\n",
        "        return self._data_store[doc_id]\n",
        "\n",
        "    @abstractmethod\n",
        "    def insert(self, text: list[Document]):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def query(self, query_text: Document) -> list[list[int]]:\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_list_frequencies(\n",
        "        data,\n",
        "        figsize=(12, 8),\n",
        "        show_top_n=10,\n",
        "        save_path=None,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        # Extract all lists and convert to tuples for counting\n",
        "        all_lists = []\n",
        "        for list_of_lists in data.values():\n",
        "            for sublist in list_of_lists:\n",
        "                all_lists.append(tuple(sublist))\n",
        "\n",
        "        if not all_lists:\n",
        "            print(\"No data found in the input dictionary.\")\n",
        "            return {}\n",
        "\n",
        "        # Count frequencies of each unique list\n",
        "        list_freq = Counter(all_lists)\n",
        "\n",
        "        # Count the frequency of frequencies\n",
        "        freq_of_freq = Counter(list_freq.values())\n",
        "\n",
        "        # Create the figure and axes\n",
        "        fig, axes = plt.subplots(2, 1, figsize=figsize)\n",
        "\n",
        "        # Plot 1: Frequency of frequencies\n",
        "        frequencies = sorted(freq_of_freq.keys())\n",
        "        counts = [freq_of_freq[f] for f in frequencies]\n",
        "\n",
        "        axes[0].bar(\n",
        "            frequencies, counts, color=\"lightblue\", edgecolor=\"black\", alpha=0.7\n",
        "        )\n",
        "        axes[0].set_xlabel(\"ID Count\", fontsize=12)\n",
        "        axes[0].set_ylabel(\"Number of lists\", fontsize=12)\n",
        "        axes[0].set_title(\"Distribution of List Frequencies\", fontsize=14)\n",
        "        axes[0].grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "        axes[0].set_xticks(frequencies)\n",
        "\n",
        "        # Plot 2: Histogram of all frequency values\n",
        "        all_frequency_values = list(list_freq.values())\n",
        "        max_freq = max(all_frequency_values)\n",
        "        axes[1].hist(\n",
        "            all_frequency_values,\n",
        "            bins=range(1, max_freq + 2),\n",
        "            rwidth=0.8,\n",
        "            align=\"left\",\n",
        "            color=\"lightcoral\",\n",
        "            edgecolor=\"black\",\n",
        "            alpha=0.7,\n",
        "        )\n",
        "        axes[1].set_xlabel(\"Frequency\", fontsize=12)\n",
        "        axes[1].set_ylabel(\"Count\", fontsize=12)\n",
        "        axes[1].set_title(\"Histogram of List Frequencies\", fontsize=14)\n",
        "        axes[1].grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "        axes[1].set_xticks(range(1, max_freq + 1))\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save or show the plot\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "            print(f\"Plot saved to {save_path}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "        if verbose:\n",
        "            # Calculate statistics\n",
        "            stats = {\n",
        "                \"total_lists\": len(all_lists),\n",
        "                \"unique_lists\": len(list_freq),\n",
        "                \"lists_appearing_once\": freq_of_freq.get(1, 0),\n",
        "                \"lists_appearing_multiple\": sum(\n",
        "                    count for freq, count in freq_of_freq.items() if freq > 1\n",
        "                ),\n",
        "                \"max_frequency\": max(all_frequency_values),\n",
        "                \"frequency_distribution\": dict(freq_of_freq),\n",
        "            }\n",
        "\n",
        "            # Print statistics\n",
        "            print(\"Summary Statistics:\")\n",
        "            print(f\"Total number of lists: {stats['total_lists']}\")\n",
        "            print(f\"Number of unique lists: {stats['unique_lists']}\")\n",
        "            print(f\"Lists appearing once: {stats['lists_appearing_once']}\")\n",
        "            print(\n",
        "                f\"Lists appearing more than once: {stats['lists_appearing_multiple']}\"\n",
        "            )\n",
        "            print(f\"Maximum frequency: {stats['max_frequency']}\")\n",
        "\n",
        "            # Show top frequent lists\n",
        "            if len(list_freq) <= show_top_n * 2:\n",
        "                print(\"\\nAll unique lists and their frequencies:\")\n",
        "                for i, (unique_list, count) in enumerate(\n",
        "                    sorted(list_freq.items(), key=lambda x: x[1], reverse=True), 1\n",
        "                ):\n",
        "                    print(f\"{i:2d}: {list(unique_list)} appears {count} time(s)\")\n",
        "            else:\n",
        "                print(f\"\\nTop {show_top_n} most frequent lists:\")\n",
        "                for i, (unique_list, count) in enumerate(\n",
        "                    sorted(list_freq.items(), key=lambda x: x[1], reverse=True)[\n",
        "                        :show_top_n\n",
        "                    ],\n",
        "                    1,\n",
        "                ):\n",
        "                    print(f\"{i:2d}: {list(unique_list)} appears {count} time(s)\")\n",
        "\n",
        "            print(\"\\nFrequency distribution:\")\n",
        "            for freq in sorted(freq_of_freq.keys()):\n",
        "                print(f\"{freq_of_freq[freq]} lists appear exactly {freq} time(s)\")\n",
        "\n",
        "            return stats\n",
        "\n",
        "\n",
        "class PrefixTreeNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.doc_ids = set()\n",
        "\n",
        "\n",
        "class Prompt:\n",
        "    def __init__(self, before, after):\n",
        "        self.before = before\n",
        "        self.after = after\n",
        "\n",
        "    def format(self, text):\n",
        "        return self.before + text + self.after\n",
        "\n",
        "\n",
        "class SequencePrefixTreeIndexStore(IndexStoreTemplate):\n",
        "    def __init__(\n",
        "        self,\n",
        "        transformer,\n",
        "        id_len,\n",
        "        universe,\n",
        "        doc_prompt: Prompt,\n",
        "        query_prompt: Prompt,\n",
        "        verbose=False,\n",
        "        initial_capacity=1000,\n",
        "        insertion_depth=3,\n",
        "        mode=\"document_search\",\n",
        "    ):\n",
        "        super().__init__(initial_capacity)\n",
        "\n",
        "        assert mode in [\"duplicate_detection\", \"document_search\"]\n",
        "        self.mode = mode\n",
        "\n",
        "        self.doc_prompt = doc_prompt\n",
        "        self.query_prompt = query_prompt\n",
        "\n",
        "        # Model for generating indices for inserted documens\n",
        "        self.transformer: GenXTransformer = transformer\n",
        "        self.id_len = id_len\n",
        "        self.universe = set(universe)\n",
        "\n",
        "        # Verbose\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Prefix tree\n",
        "        self.root = PrefixTreeNode()\n",
        "        self.insertion_depth = insertion_depth\n",
        "\n",
        "    def save_state(self, filepath):\n",
        "        import joblib\n",
        "\n",
        "        # Create a state dictionary with all attributes except transformer\n",
        "        state = {\n",
        "            # Capacity and sizing\n",
        "            \"_initial_capacity\": self._initial_capacity,\n",
        "            \"capacity\": self.capacity,\n",
        "            \"next_id\": self.next_id,\n",
        "            \"size\": self.size,\n",
        "            # Data storage\n",
        "            \"_ids\": self._ids,\n",
        "            \"_data_store\": self._data_store,\n",
        "            \"_beams_store\": self._beams_store,\n",
        "            \"_original_keys\": self._original_keys,\n",
        "            \"_original_keys_transpose\": self._original_keys_transpose,\n",
        "            # Configuration\n",
        "            \"mode\": self.mode,\n",
        "            \"doc_prompt\": self.doc_prompt,\n",
        "            \"query_prompt\": self.query_prompt,\n",
        "            \"id_len\": self.id_len,\n",
        "            \"universe\": self.universe,\n",
        "            \"verbose\": self.verbose,\n",
        "            \"insertion_depth\": self.insertion_depth,\n",
        "            # Prefix tree\n",
        "            \"root\": self.root,\n",
        "        }\n",
        "\n",
        "        # Save using joblib\n",
        "        joblib.dump(state, filepath)\n",
        "        print(f\"State saved to {filepath}\")\n",
        "\n",
        "    def load_state(self, filepath):\n",
        "        import joblib\n",
        "\n",
        "        # Load the state\n",
        "        state = joblib.load(filepath)\n",
        "\n",
        "        # Restore all attributes\n",
        "        self._initial_capacity = state[\"_initial_capacity\"]\n",
        "        self.capacity = state[\"capacity\"]\n",
        "        self.next_id = state[\"next_id\"]\n",
        "        self.size = state[\"size\"]\n",
        "        self._ids = state[\"_ids\"]\n",
        "        self._data_store = state[\"_data_store\"]\n",
        "        self._beams_store = state[\"_beams_store\"]\n",
        "        self._original_keys = state[\"_original_keys\"]\n",
        "        self._original_keys_transpose = state[\"_original_keys_transpose\"]\n",
        "        self.mode = state[\"mode\"]\n",
        "        self.doc_prompt = state[\"doc_prompt\"]\n",
        "        self.query_prompt = state[\"query_prompt\"]\n",
        "        self.id_len = state[\"id_len\"]\n",
        "        self.universe = state[\"universe\"]\n",
        "        self.verbose = state[\"verbose\"]\n",
        "        self.insertion_depth = state[\"insertion_depth\"]\n",
        "        self.root = state[\"root\"]\n",
        "\n",
        "        print(f\"State loaded from {filepath}\")\n",
        "\n",
        "    def print_data(self):\n",
        "        for id, val in self._data_store.items():\n",
        "            print(val.get_text(), val.get_metadata())\n",
        "            print(self._beams_store[id])\n",
        "            print()\n",
        "\n",
        "    def print_beams_store(self):\n",
        "        for id, val in self._beams_store.items():\n",
        "            print(\n",
        "                id,\n",
        "                val,\n",
        "                self.transformer.inference_tokenizer.batch_decode(\n",
        "                    val, skip_special_tokens=False\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def set_verbose_for_all(self, verbose):\n",
        "        self.verbose = verbose\n",
        "        if hasattr(self.transformer, \"verbose\"):\n",
        "            self.transformer.verbose = verbose\n",
        "\n",
        "    def reset_id_len(self, id_len):\n",
        "        self.id_len = id_len\n",
        "        self.transformer.update_num_next_tokens(max_new_tokens=id_len)\n",
        "\n",
        "    def clear_store(self):\n",
        "        self.root = PrefixTreeNode()\n",
        "\n",
        "        super()._clear_store()\n",
        "        if self.verbose:\n",
        "            print(f\"Store cleared, current capacity: {self.capacity}\")\n",
        "\n",
        "    def _insert_document(self, texts: list[Document], prompt_template):\n",
        "        if not isinstance(texts, list):\n",
        "            texts = [texts]\n",
        "\n",
        "        self._resize_if_needed(len(texts))\n",
        "\n",
        "        doc_ids = []\n",
        "        template_texts = []\n",
        "        for text in texts:\n",
        "            doc_id = self.next_id\n",
        "            doc_ids.append(doc_id)\n",
        "            # Update index in data store\n",
        "            self.next_id += 1\n",
        "            self.size += 1\n",
        "\n",
        "            # Save text in data store\n",
        "            self._ids[self.size - 1] = doc_id\n",
        "            self._data_store[doc_id] = text\n",
        "            self._original_keys[doc_id] = text.get_metadata()[\"doc_id\"]\n",
        "            self._original_keys_transpose[text.get_metadata()[\"doc_id\"]] = doc_id\n",
        "\n",
        "            template_text = prompt_template(text.get_text())\n",
        "            template_texts.append(template_text)\n",
        "\n",
        "        # Generate beams of sequences\n",
        "        # [batch_size, num_return_sequences, sequence_length]\n",
        "        lst_of_sequences = self.transformer.index_doc(template_texts)\n",
        "        print(lst_of_sequences) if self.verbose else None\n",
        "        self._insert_sequences_into_tree(lst_of_sequences, doc_ids)\n",
        "\n",
        "    def _insert_sequences_into_tree(\n",
        "        self, lst_of_sequences: list[list[list[int]]], doc_ids: list[int]\n",
        "    ):\n",
        "        for sequences, doc_id in zip(lst_of_sequences, doc_ids):\n",
        "            # Store sequences of a doc in to database\n",
        "            self._beams_store[doc_id] = sequences\n",
        "\n",
        "            for seq in sequences:\n",
        "                print(f\"Tokens: {seq}\") if self.verbose else None\n",
        "                if len(seq) != self.id_len or not all(x in self.universe for x in seq):\n",
        "                    continue  # Skip invalid sequences\n",
        "\n",
        "                self._traverse_and_insert(seq, doc_id)\n",
        "\n",
        "    def _traverse_and_insert(self, seq, doc_id):\n",
        "        node = self.root\n",
        "        depth = 0\n",
        "\n",
        "        for idx in seq:\n",
        "            if idx not in node.children:\n",
        "                node.children[idx] = PrefixTreeNode()\n",
        "            node = node.children[idx]\n",
        "            depth += 1\n",
        "            if depth >= self.insertion_depth:\n",
        "                node.doc_ids.add(doc_id)\n",
        "                if self.verbose:\n",
        "                    print(f\"Inserted doc {doc_id} at depth {depth} of prefix tree\")\n",
        "\n",
        "    def insert(self, texts: list[Document]):\n",
        "        _texts = []\n",
        "        for text in texts:\n",
        "            doc_id = text.get_metadata()[\"doc_id\"]\n",
        "            if doc_id not in self._original_keys.values():\n",
        "                _texts.append(text)\n",
        "        if len(_texts) > 0:\n",
        "            print(f\"Inserting '{_texts}'\") if self.verbose else None\n",
        "            self._insert_document(_texts, self.doc_prompt.format)\n",
        "\n",
        "    def _query_with_prompt(\n",
        "        self,\n",
        "        query_texts: list[Document],\n",
        "        prompt_template,\n",
        "    ) -> list[list[dict]]:\n",
        "        if not query_texts:\n",
        "            return []\n",
        "\n",
        "        lst_of_result_ids = []\n",
        "        template_texts = []\n",
        "        for query_text in query_texts:\n",
        "            template_text = prompt_template(query_text.get_text())\n",
        "            print(template_text) if self.verbose else None\n",
        "            template_texts.append(template_text)\n",
        "\n",
        "        # [batch_size, num_return_sequences, sequence_length]\n",
        "        lst_of_sequences = self.transformer.index_query(template_texts)\n",
        "        for sequences in lst_of_sequences:\n",
        "            result_ids = []\n",
        "            for seq in sequences:\n",
        "                print(f\"Tokens: {seq}\") if self.verbose else None\n",
        "                if len(seq) != self.id_len or not all(x in self.universe for x in seq):\n",
        "                    continue\n",
        "\n",
        "                result = self._traverse_tree_for_query(seq)\n",
        "                if result:\n",
        "                    result[\"index_ids\"] = seq\n",
        "                    result[\"index_txt\"] = self.transformer.inference_tokenizer.batch_decode(\n",
        "                        seq, skip_special_tokens=False\n",
        "                    )\n",
        "                    result_ids.append(result)\n",
        "            print(\"Found results: \", result_ids) if self.verbose else None\n",
        "            lst_of_result_ids.append(result_ids)\n",
        "\n",
        "        return lst_of_result_ids\n",
        "\n",
        "    def _traverse_tree_for_query(self, seq):\n",
        "        node: PrefixTreeNode = self.root\n",
        "        found = True\n",
        "        depth = 0\n",
        "\n",
        "        for idx in seq:\n",
        "            if (idx not in node.children) and (depth < self.insertion_depth):\n",
        "                found = False\n",
        "                break\n",
        "            if (idx not in node.children) and (depth >= self.insertion_depth):\n",
        "                break\n",
        "            node = node.children[idx]\n",
        "            depth += 1\n",
        "\n",
        "        if found:\n",
        "            return {\"depth\": depth, \"doc_ids\": node.doc_ids}\n",
        "        return None\n",
        "\n",
        "    def query(self, query_texts: list[Document]):\n",
        "        print(f\"Querying for '{query_texts}'\") if self.verbose else None\n",
        "        return self._query_with_prompt(query_texts, self.query_prompt.format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################## Train ########################################\n",
        "\n",
        "\n",
        "def log_validation(\n",
        "    store,\n",
        "    scifact_dataloader,\n",
        "    accelerator,\n",
        "    epoch,\n",
        "    split: str,\n",
        "    global_step: int,\n",
        "    is_final_validation: bool = False,\n",
        "):\n",
        "    cited_doc_ids = []\n",
        "    results = []\n",
        "\n",
        "    for batch in scifact_dataloader:\n",
        "        # Use dict to automatically handle duplicates (keeps last occurrence)\n",
        "        unique_queries = {}\n",
        "        for query, doc_id in zip(batch[\"query\"], batch[\"doc_id\"]):\n",
        "            if query not in unique_queries:\n",
        "                unique_queries[query] = []\n",
        "            unique_queries[query].append(doc_id)\n",
        "\n",
        "        queries_to_be_queried = []\n",
        "        doc_ids_for_a_query = []\n",
        "        for query, doc_ids in unique_queries.items():\n",
        "            queries_to_be_queried.append(Document(query, {\"doc_ids\": doc_ids}))\n",
        "            doc_ids_for_a_query.append(doc_ids)\n",
        "\n",
        "        result = store.query(queries_to_be_queried)\n",
        "        results.extend(result)\n",
        "        cited_doc_ids.extend(doc_ids_for_a_query)\n",
        "\n",
        "    assert len(results) == len(cited_doc_ids)\n",
        "\n",
        "    total_predicted = 0\n",
        "    total_correctly_predicted = 0\n",
        "    total_gold = 0\n",
        "\n",
        "    for idx in range(len(results)):\n",
        "        result = results[idx]\n",
        "        cited_doc_id = cited_doc_ids[idx]\n",
        "\n",
        "        # Extract predicted document IDs\n",
        "        if len(result) > 0:\n",
        "            temp = []\n",
        "            for item in result:\n",
        "                temp.extend(item[\"doc_ids\"])\n",
        "            predicted = set(temp)\n",
        "        else:\n",
        "            predicted = set()\n",
        "\n",
        "        predicted = list(predicted)\n",
        "        for idx, pred in enumerate(predicted):\n",
        "            predicted[idx] = int(store._data_store[pred].get_metadata()[\"doc_id\"])\n",
        "\n",
        "        cited_doc_id = set(cited_doc_id)\n",
        "        predicted = set(predicted)\n",
        "\n",
        "        # Count metrics\n",
        "        total_predicted += len(predicted)\n",
        "        total_gold += len(cited_doc_id)\n",
        "\n",
        "        # Count correctly predicted abstracts (intersection)\n",
        "        correctly_predicted = predicted.intersection(set(cited_doc_id))\n",
        "        total_correctly_predicted += len(correctly_predicted)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    if total_predicted > 0:\n",
        "        precision = total_correctly_predicted / total_predicted\n",
        "    else:\n",
        "        precision = 0.0\n",
        "\n",
        "    if total_gold > 0:\n",
        "        recall = total_correctly_predicted / total_gold\n",
        "    else:\n",
        "        recall = 0.0\n",
        "\n",
        "    # Calculate F1 score\n",
        "    if precision + recall > 0:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    else:\n",
        "        f1_score = 0.0\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        \"epoch\": epoch,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1_score,\n",
        "    }\n",
        "\n",
        "    # Log results\n",
        "    flag = split\n",
        "    if is_final_validation:\n",
        "        flag = f\"final-{split}\"\n",
        "    for tracker in accelerator.trackers:\n",
        "        if tracker.name == \"wandb\":\n",
        "            tracker.log(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    f\"{flag}-precision\": precision,\n",
        "                    f\"{flag}-recall\": recall,\n",
        "                    f\"{flag}-f1\": f1_score,\n",
        "                },\n",
        "                step=global_step,\n",
        "            )\n",
        "    return metrics\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Arguments:\n",
        "    # Model\n",
        "    query_model_alias: str = field(default=\"llama-3.2-1b-instruct\")\n",
        "    doc_model_alias: str = field(default=\"llama-3.2-1b-instruct\")\n",
        "    train_max_length: int = field(default=128)\n",
        "    inference_max_length: int = field(default=2048)\n",
        "    torch_dtype: str = field(default=\"bfloat16\")\n",
        "\n",
        "    # Prompts\n",
        "    doc_prompt_before: str = field(default=\"\")\n",
        "    doc_prompt_after: str = field(\n",
        "        default=\"Give me keywords. Do NOT use bullet points, numbered lists, or line breaks. Keep the output as one continuous block of text.\"\n",
        "    )\n",
        "    query_prompt_before: str = field(default=\"\")\n",
        "    query_prompt_after: str = field(\n",
        "        default=\"Give me potential keywords of related articles.\"\n",
        "    )\n",
        "    duplicate_prompt_before: str = field(\n",
        "        default=\"Given the text first remove all the punctuations and stop words. Then shuffle the sentences. Generate some unique related phrases that does not have synonyms. \"\n",
        "    )\n",
        "    duplicate_prompt_after: str = field(default=\" IGNORE ME. Phrases: \")\n",
        "\n",
        "    # Data\n",
        "    data_path: str = field(\n",
        "        default=\"/data/users/zy45/fbsource/fbcode/gen_ai/web_search/fbsearch/scripts/data/scifact/\"\n",
        "    )\n",
        "    train_queries_filename: str = field(default=\"claims_train_subset.jsonl\")\n",
        "    dev_queries_filename: str = field(default=\"claims_dev_subset.jsonl\")\n",
        "    syn_queries_filename: str = field(default=\"claims_syn_all_dedup.jsonl\")\n",
        "    corpus_filename: str = field(default=\"corpus_subset.jsonl\")\n",
        "    store_state_filename: str = field(default=\"store.joblib\")\n",
        "    output_dir: str = field(\n",
        "        default=\"/data/users/zy45/fbsource/fbcode/gen_ai/web_search/fbsearch/scripts/runs/scifact/train-on-real-sub-3\"\n",
        "    )\n",
        "\n",
        "    # Loading\n",
        "    load_store_state: bool = field(default=False)\n",
        "\n",
        "    # Logging\n",
        "    logging_dir: str = field(default=\"logs\")\n",
        "    tracker_name: str = field(default=\"scifact\")\n",
        "\n",
        "    # Device\n",
        "    device: str = field(default=\"cuda\")\n",
        "\n",
        "    # Seed\n",
        "    seed: int = field(default=0)\n",
        "\n",
        "    # Accelerator\n",
        "    mixed_precision: str = field(default=\"bf16\")\n",
        "    report_to: str = field(default=\"wandb\")\n",
        "    do_report: bool = field(default=False)\n",
        "\n",
        "    # Indexing\n",
        "    num_beams: int = field(default=1)\n",
        "    num_next_tokens: int = field(default=3)\n",
        "    insertion_depth: int = field(default=3)\n",
        "\n",
        "    # Training\n",
        "    per_device_train_batch_size: int = field(default=16)\n",
        "    lr_warmup_steps: int = field(default=100)\n",
        "    lr_scheduler: str = field(default=\"cosine\")\n",
        "    gradient_accumulation_steps: int = field(default=1)\n",
        "    num_train_epochs: int = field(default=20)\n",
        "    learning_rate: float = field(default=5e-5)\n",
        "    adam_beta1: float = field(default=0.9)\n",
        "    adam_beta2: float = field(default=0.999)\n",
        "    adam_epsilon: float = field(default=1e-8)\n",
        "    adam_weight_decay: float = field(default=0.0)\n",
        "    max_grad_norm: float = field(default=1.0)\n",
        "    dataloader_num_workers: int = field(default=4)\n",
        "    validation_epochs: int = field(default=20)\n",
        "    train_on_syn_data: bool = field(default=False)\n",
        "\n",
        "    # Resume checkpoint\n",
        "    resume_from_checkpoint: str = field(default=None)\n",
        "    checkpointing_epochs: int = field(default=99999)\n",
        "\n",
        "\n",
        "logger = get_logger(__name__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "output": {
          "id": 24599714383049104,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:13 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: bf16\n",
            "\n",
            "09/19/2025 17:22:13 - INFO - __main__ - Inserting corpus into data store...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DDP training\n",
            "Created a repo /data/users/zy45/fbsource/fbcode/gen_ai/web_search/fbsearch/scripts/runs/scifact/train-on-real-sub-3\n",
            "{'max_new_tokens': 3, 'do_sample': False, 'num_beams': 1, 'num_return_sequences': 1, 'eos_token_id': None}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 [[315, 3823, 46397]] [' of human newborn']\n",
            "1 [[856, 301, 347]] [' myelod']\n",
            "2 [[41214, 11, 279]] [' RNA, the']\n",
            "3 [[42972, 638, 315]] ['ethylome of']\n",
            "\n",
            "State saved to /data/users/zy45/fbsource/fbcode/gen_ai/web_search/fbsearch/scripts/runs/scifact/train-on-real-sub-3/store.joblib\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:15 - INFO - __main__ - ***** Running training *****\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:15 - INFO - __main__ - Num examples = 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:15 - INFO - __main__ - Num batches each epoch = 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:15 - INFO - __main__ - Num epochs = 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:15 - INFO - __main__ - Instantaneous batch size per device = 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:15 - INFO - __main__ - Total train batch size (w. parallel, distributed & accumulation) = 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:15 - INFO - __main__ - Gradient accumulation steps = 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:15 - INFO - __main__ - Number of update steps per epoch = 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:15 - INFO - __main__ - Total optimization steps = 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:15 - INFO - __main__ - Checkpointing epochs = 99999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09/19/2025 17:22:15 - INFO - __main__ - Validation epochs = 20\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plot saved to /data/users/zy45/fbsource/fbcode/gen_ai/web_search/fbsearch/scripts/runs/scifact/train-on-real-sub-3/freq.pdf\n",
            "Summary Statistics:\n",
            "Total number of lists: 4\n",
            "Number of unique lists: 4\n",
            "Lists appearing once: 4\n",
            "Lists appearing more than once: 0\n",
            "Maximum frequency: 1\n",
            "\n",
            "All unique lists and their frequencies:\n",
            " 1: [315, 3823, 46397] appears 1 time(s)\n",
            " 2: [856, 301, 347] appears 1 time(s)\n",
            " 3: [41214, 11, 279] appears 1 time(s)\n",
            " 4: [42972, 638, 315] appears 1 time(s)\n",
            "\n",
            "Frequency distribution:\n",
            "4 lists appear exactly 1 time(s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps: 100%|| 20/20 [10:49<00:00, 32.46s/it, loss=1.39e-5, lr=1e-5]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:   5%|         | 1/20 [00:00<00:07,  2.67it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:   5%|         | 1/20 [00:00<00:07,  2.67it/s, loss=1.17e-5, lr=5e-7]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  10%|         | 2/20 [00:01<00:15,  1.18it/s, loss=1.17e-5, lr=5e-7]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  10%|         | 2/20 [00:01<00:15,  1.18it/s, loss=1.17e-5, lr=1e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  15%|        | 3/20 [00:01<00:11,  1.54it/s, loss=1.17e-5, lr=1e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  15%|        | 3/20 [00:01<00:11,  1.54it/s, loss=1.19e-5, lr=1.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  20%|        | 4/20 [00:02<00:08,  1.78it/s, loss=1.19e-5, lr=1.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  20%|        | 4/20 [00:02<00:08,  1.78it/s, loss=9.94e-6, lr=2e-6]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  25%|       | 5/20 [00:02<00:07,  1.98it/s, loss=9.94e-6, lr=2e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  25%|       | 5/20 [00:02<00:07,  1.98it/s, loss=8.6e-6, lr=2.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  30%|       | 6/20 [00:03<00:06,  2.15it/s, loss=8.6e-6, lr=2.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  30%|       | 6/20 [00:03<00:06,  2.15it/s, loss=6.66e-6, lr=3e-6] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  35%|      | 7/20 [00:03<00:05,  2.27it/s, loss=6.66e-6, lr=3e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  35%|      | 7/20 [00:03<00:05,  2.27it/s, loss=4.31e-6, lr=3.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  40%|      | 8/20 [00:03<00:05,  2.34it/s, loss=4.31e-6, lr=3.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  40%|      | 8/20 [00:04<00:05,  2.34it/s, loss=2.94e-6, lr=4e-6]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  45%|     | 9/20 [00:04<00:04,  2.37it/s, loss=2.94e-6, lr=4e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  45%|     | 9/20 [00:04<00:04,  2.37it/s, loss=2.19e-6, lr=4.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  50%|     | 10/20 [00:04<00:04,  2.39it/s, loss=2.19e-6, lr=4.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  50%|     | 10/20 [00:04<00:04,  2.39it/s, loss=1.77e-6, lr=5e-6]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  55%|    | 11/20 [00:05<00:03,  2.41it/s, loss=1.77e-6, lr=5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  55%|    | 11/20 [00:05<00:03,  2.41it/s, loss=1.26e-6, lr=5.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  60%|    | 12/20 [00:05<00:03,  2.24it/s, loss=1.26e-6, lr=5.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  60%|    | 12/20 [00:05<00:03,  2.24it/s, loss=9.93e-7, lr=6e-6]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  65%|   | 13/20 [00:06<00:03,  2.31it/s, loss=9.93e-7, lr=6e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  65%|   | 13/20 [00:06<00:03,  2.31it/s, loss=8.64e-7, lr=6.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  70%|   | 14/20 [00:06<00:02,  2.36it/s, loss=8.64e-7, lr=6.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  70%|   | 14/20 [00:06<00:02,  2.36it/s, loss=6.85e-7, lr=7e-6]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  75%|  | 15/20 [00:06<00:02,  2.39it/s, loss=6.85e-7, lr=7e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  75%|  | 15/20 [00:06<00:02,  2.39it/s, loss=5.86e-7, lr=7.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  80%|  | 16/20 [00:07<00:01,  2.38it/s, loss=5.86e-7, lr=7.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  80%|  | 16/20 [00:07<00:01,  2.38it/s, loss=4.77e-7, lr=8e-6]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  85%| | 17/20 [00:07<00:01,  2.41it/s, loss=4.77e-7, lr=8e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  85%| | 17/20 [00:07<00:01,  2.41it/s, loss=3.48e-7, lr=8.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  90%| | 18/20 [00:08<00:00,  2.44it/s, loss=3.48e-7, lr=8.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  90%| | 18/20 [00:08<00:00,  2.44it/s, loss=2.38e-7, lr=9e-6]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  95%|| 19/20 [00:08<00:00,  2.38it/s, loss=2.38e-7, lr=9e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps:  95%|| 19/20 [00:08<00:00,  2.38it/s, loss=1.89e-7, lr=9.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps: 100%|| 20/20 [00:09<00:00,  2.41it/s, loss=1.89e-7, lr=9.5e-6]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rSteps: 100%|| 20/20 [00:09<00:00,  2.41it/s, loss=1.59e-7, lr=1e-5]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[A"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "args = Arguments()\n",
        "\n",
        "if args:\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    print(\"DDP training\")\n",
        "\n",
        "    # Accelerator\n",
        "    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
        "    accelerator_project_config = ProjectConfiguration(\n",
        "        project_dir=args.output_dir, logging_dir=logging_dir\n",
        "    )\n",
        "    kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        mixed_precision=args.mixed_precision,\n",
        "        log_with=args.report_to,\n",
        "        project_config=accelerator_project_config,\n",
        "        kwargs_handlers=[kwargs],\n",
        "    )\n",
        "\n",
        "    # Make one log on every process with the configuration for debugging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    logger.info(accelerator.state, main_process_only=False)\n",
        "    if accelerator.is_local_main_process:\n",
        "        transformers_utils_logging.set_verbosity_warning()\n",
        "    else:\n",
        "        transformers_utils_logging.set_verbosity_error()\n",
        "\n",
        "    # Seed\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if accelerator.is_main_process:\n",
        "        if args.output_dir is not None:\n",
        "            os.makedirs(args.output_dir, exist_ok=True)\n",
        "            print(f\"Created a repo {args.output_dir}\")\n",
        "\n",
        "    # Data\n",
        "    data_path = args.data_path\n",
        "    train_queries_path = os.path.join(data_path, args.train_queries_filename)\n",
        "    syn_queries_path = os.path.join(data_path, args.syn_queries_filename)\n",
        "    dev_queries_path = os.path.join(data_path, args.dev_queries_filename)\n",
        "    corpus_path = os.path.join(data_path, args.corpus_filename)\n",
        "\n",
        "    per_device_train_batch_size = args.per_device_train_batch_size\n",
        "\n",
        "    # Prompt templates\n",
        "    corpus_prompt_template = Prompt(\n",
        "        before=args.doc_prompt_before,\n",
        "        after=args.doc_prompt_after,\n",
        "    )\n",
        "    query_prompt_template = Prompt(\n",
        "        before=args.query_prompt_before,\n",
        "        after=args.query_prompt_after,\n",
        "    )\n",
        "\n",
        "    # Corpus\n",
        "    corpus_dataset = SciFactCorpusDataset(\n",
        "        corpus_path,\n",
        "    )\n",
        "    corpus_dataloader = DataLoader(\n",
        "        corpus_dataset,\n",
        "        batch_size=per_device_train_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=args.dataloader_num_workers,\n",
        "    )\n",
        "    corpus_dict = corpus_dataset.get_corpus_dict()\n",
        "\n",
        "    # Queries\n",
        "    real_train_dataloader = get_scifact_query_dataloader(\n",
        "        queries_path=train_queries_path,\n",
        "        corpus_dict=corpus_dict,\n",
        "        batch_size=per_device_train_batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args.dataloader_num_workers,\n",
        "    )\n",
        "    if args.train_on_syn_data:\n",
        "        train_dataloader = get_scifact_query_dataloader(\n",
        "            queries_path=[syn_queries_path, train_queries_path],\n",
        "            corpus_dict=corpus_dict,\n",
        "            batch_size=per_device_train_batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=args.dataloader_num_workers,\n",
        "        )\n",
        "    else:\n",
        "        train_dataloader = real_train_dataloader\n",
        "\n",
        "    dev_dataloader = get_scifact_query_dataloader(\n",
        "        queries_path=dev_queries_path,\n",
        "        corpus_dict=corpus_dict,\n",
        "        batch_size=per_device_train_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=args.dataloader_num_workers,\n",
        "    )\n",
        "\n",
        "    # Data type\n",
        "    torch_dtype = getattr(torch, args.torch_dtype)\n",
        "\n",
        "    # Model\n",
        "    if \"query_model\" not in locals():\n",
        "        print(\"Loading models...\")\n",
        "        query_model_name_or_path = TRANSFORMERS_PATH_MAP[args.query_model_alias]\n",
        "        doc_model_name_or_path = TRANSFORMERS_PATH_MAP[args.doc_model_alias]\n",
        "        if \"query_model_copy\" not in locals():\n",
        "            query_model_copy, train_tokenizer = get_model_and_tokenizer(\n",
        "                query_model_name_or_path,\n",
        "                model_max_length=args.train_max_length,\n",
        "                torch_dtype=torch_dtype,\n",
        "                padding_side=\"right\",\n",
        "            )\n",
        "        if \"doc_model\" not in locals():\n",
        "            doc_model, inference_tokenizer = get_model_and_tokenizer(\n",
        "                doc_model_name_or_path,\n",
        "                model_max_length=args.inference_max_length,\n",
        "                torch_dtype=torch_dtype,\n",
        "                padding_side=\"left\",\n",
        "            )\n",
        "            doc_model.to(accelerator.device, dtype=torch_dtype)\n",
        "\n",
        "        query_model = copy.deepcopy(query_model_copy)\n",
        "        query_model.to(accelerator.device, dtype=torch_dtype)\n",
        "\n",
        "    # Only train the query model\n",
        "    query_model.train()\n",
        "    doc_model.eval()\n",
        "    doc_model.requires_grad_(False)\n",
        "\n",
        "    # Optimizer and learning rate scheduler\n",
        "    num_warmup_steps_for_scheduler = args.lr_warmup_steps * accelerator.num_processes\n",
        "    len_train_dataloader_after_sharding = math.ceil(\n",
        "        len(train_dataloader) / accelerator.num_processes\n",
        "    )\n",
        "    num_update_steps_per_epoch = math.ceil(\n",
        "        len_train_dataloader_after_sharding / args.gradient_accumulation_steps\n",
        "    )\n",
        "    num_training_steps_for_scheduler = (\n",
        "        args.num_train_epochs * accelerator.num_processes * num_update_steps_per_epoch\n",
        "    )\n",
        "    params_to_optimize = [\n",
        "        {\n",
        "            \"params\": list(filter(lambda p: p.requires_grad, query_model.parameters())),\n",
        "            \"lr\": args.learning_rate,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        params_to_optimize,\n",
        "        betas=(args.adam_beta1, args.adam_beta2),\n",
        "        weight_decay=args.adam_weight_decay,\n",
        "        eps=args.adam_epsilon,\n",
        "    )\n",
        "    lr_scheduler = get_scheduler(\n",
        "        args.lr_scheduler,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=num_warmup_steps_for_scheduler,\n",
        "        num_training_steps=num_training_steps_for_scheduler,\n",
        "    )\n",
        "\n",
        "    # The size of the training dataloader may have changed due to accelerator.prepare, so we need to recalculate our total training steps\n",
        "    num_update_steps_per_epoch = math.ceil(\n",
        "        len(train_dataloader) / args.gradient_accumulation_steps\n",
        "    )\n",
        "    max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "    if num_training_steps_for_scheduler != max_train_steps:\n",
        "        logger.warning(\n",
        "            f\"The length of the 'train_dataloader' after 'accelerator.prepare' ({len(train_dataloader)}) does not match \"\n",
        "            f\"the expected length ({len_train_dataloader_after_sharding}) when the learning rate scheduler was created. \"\n",
        "            f\"This inconsistency may result in the learning rate scheduler not functioning properly.\"\n",
        "        )\n",
        "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    genx_transformer = GenXTransformer(\n",
        "        query_model=query_model,\n",
        "        doc_model=doc_model,\n",
        "        train_tokenizer=train_tokenizer,\n",
        "        inference_tokenizer=inference_tokenizer,\n",
        "        num_beams=args.num_beams,\n",
        "        num_next_tokens=args.num_next_tokens,\n",
        "    )\n",
        "    print(genx_transformer.genx_gen_kwargs)\n",
        "\n",
        "    # Prepare everything with our accelerator\n",
        "    (\n",
        "        query_model,\n",
        "        doc_model,\n",
        "        optimizer,\n",
        "        corpus_dataloader,\n",
        "        train_dataloader,\n",
        "        dev_dataloader,\n",
        "        lr_scheduler,\n",
        "    ) = accelerator.prepare(\n",
        "        query_model,\n",
        "        doc_model,\n",
        "        optimizer,\n",
        "        corpus_dataloader,\n",
        "        train_dataloader,\n",
        "        dev_dataloader,\n",
        "        lr_scheduler,\n",
        "    )\n",
        "\n",
        "    # Report\n",
        "    if accelerator.is_main_process and args.do_report:\n",
        "        tracker_name = args.tracker_name\n",
        "        accelerator.init_trackers(tracker_name, config=vars(args))\n",
        "\n",
        "    store = SequencePrefixTreeIndexStore(\n",
        "        genx_transformer,\n",
        "        id_len=args.num_next_tokens,\n",
        "        universe=set(range(genx_transformer.inference_tokenizer.vocab_size)),\n",
        "        doc_prompt=corpus_prompt_template,\n",
        "        query_prompt=query_prompt_template,\n",
        "        mode=\"document_search\",\n",
        "        insertion_depth=args.insertion_depth,\n",
        "    )\n",
        "    store.clear_store()\n",
        "    store.set_verbose_for_all(False)\n",
        "\n",
        "    store_state_path = os.path.join(args.output_dir, args.store_state_filename)\n",
        "    if args.load_store_state and os.path.exists(store_state_path):\n",
        "        logger.info(\"Loading state...\")\n",
        "        store.load_state(store_state_path)\n",
        "        if len(store._beams_store) < 10:\n",
        "            store.print_beams_store()\n",
        "            print()\n",
        "        store.plot_list_frequencies(\n",
        "            store._beams_store,\n",
        "            figsize=(12, 8),\n",
        "            save_path=os.path.join(args.output_dir, \"freq.pdf\"),\n",
        "            verbose=True if accelerator.is_main_process else False,\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Inserting corpus into data store...\")\n",
        "        for batch in corpus_dataloader:\n",
        "            doc_ids = batch[\"doc_id\"]\n",
        "            texts = batch[\"text\"]\n",
        "            to_be_inserted = []\n",
        "            for doc_id, text in zip(doc_ids, texts):\n",
        "                doc_id = doc_id.cpu().item()\n",
        "                to_be_inserted.append(Document(text, {\"doc_id\": doc_id}))\n",
        "            store.insert(to_be_inserted)\n",
        "        if len(store._beams_store) < 10:\n",
        "            store.print_beams_store()\n",
        "            print()\n",
        "        store.save_state(store_state_path)\n",
        "        store.plot_list_frequencies(\n",
        "            store._beams_store,\n",
        "            figsize=(12, 8),\n",
        "            save_path=os.path.join(args.output_dir, \"freq.pdf\"),\n",
        "            verbose=True if accelerator.is_main_process else False,\n",
        "        )\n",
        "\n",
        "    # Log some info about our training\n",
        "    total_batch_size = (\n",
        "        per_device_train_batch_size\n",
        "        * accelerator.num_processes\n",
        "        * args.gradient_accumulation_steps\n",
        "    )\n",
        "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"Num examples = {len(train_dataloader.dataset)}\")\n",
        "    logger.info(f\"Num batches each epoch = {len(train_dataloader)}\")\n",
        "    logger.info(f\"Num epochs = {num_train_epochs}\")\n",
        "    logger.info(f\"Instantaneous batch size per device = {per_device_train_batch_size}\")\n",
        "    logger.info(\n",
        "        f\"Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
        "    )\n",
        "    logger.info(f\"Gradient accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "    logger.info(f\"Number of update steps per epoch = {num_update_steps_per_epoch}\")\n",
        "    logger.info(f\"Total optimization steps = {max_train_steps}\")\n",
        "    logger.info(f\"Checkpointing epochs = {args.checkpointing_epochs}\")\n",
        "    logger.info(f\"Validation epochs = {args.validation_epochs}\")\n",
        "    checkpointing_steps = args.checkpointing_epochs * num_update_steps_per_epoch\n",
        "    global_step = 0\n",
        "    first_epoch = 0\n",
        "\n",
        "    best_dev_f1 = 0.0\n",
        "    best_train_metrics = {}\n",
        "    best_dev_metrics = {}\n",
        "\n",
        "    # Potentially load in the weights and states from a previous save\n",
        "    if args.resume_from_checkpoint:\n",
        "        if args.resume_from_checkpoint != \"latest\":\n",
        "            path = os.path.basename(args.resume_from_checkpoint)\n",
        "        else:\n",
        "            # Get the mos recent checkpoint\n",
        "            dirs = os.listdir(args.output_dir)\n",
        "            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
        "            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
        "            path = dirs[-1] if len(dirs) > 0 else None\n",
        "\n",
        "        if path is None:\n",
        "            accelerator.print(\n",
        "                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
        "            )\n",
        "            initial_global_step = 0\n",
        "        else:\n",
        "            accelerator.print(f\"Resuming from checkpoint {path}\")\n",
        "            accelerator.load_state(os.path.join(args.output_dir, path))\n",
        "            global_step = int(path.split(\"-\")[1])\n",
        "\n",
        "            initial_global_step = global_step\n",
        "            first_epoch = global_step // num_update_steps_per_epoch\n",
        "    else:\n",
        "        initial_global_step = 0\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        range(0, max_train_steps),\n",
        "        initial=initial_global_step,\n",
        "        desc=\"Steps\",\n",
        "        # Only show the progress bar once on each machine.\n",
        "        disable=not accelerator.is_local_main_process,\n",
        "    )\n",
        "    epoch = first_epoch\n",
        "    for epoch in range(first_epoch, num_train_epochs):\n",
        "        query_model.train()\n",
        "        for _, batch in enumerate(train_dataloader):\n",
        "            models_to_accumulate = [query_model]\n",
        "            with accelerator.accumulate(models_to_accumulate):\n",
        "                doc_ids = batch[\"doc_id\"]\n",
        "                queries = batch[\"query\"]\n",
        "                docs = batch[\"text\"]\n",
        "                docs = [corpus_prompt_template.format(text) for text in docs]\n",
        "                queries = [query_prompt_template.format(query) for query in queries]\n",
        "\n",
        "                beams_for_docs = []\n",
        "                all_processed = True\n",
        "                for doc_id in doc_ids:\n",
        "                    if doc_id not in store._original_keys.values():\n",
        "                        all_processed = False\n",
        "                        break\n",
        "                if all_processed:\n",
        "                    for doc_id in doc_ids:\n",
        "                        id = store._original_keys_transpose[doc_id]\n",
        "                        beams_for_docs.append(store._beams_store[id])\n",
        "                    loss = genx_transformer(queries, beams_for_docs)\n",
        "                else:\n",
        "                    loss = genx_transformer(queries, docs)\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                if accelerator.sync_gradients:\n",
        "                    params_to_clip = itertools.chain(query_model.parameters())\n",
        "                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "\n",
        "                if accelerator.is_main_process:\n",
        "                    if global_step % checkpointing_steps == 0:\n",
        "                        save_path = os.path.join(\n",
        "                            args.output_dir, f\"checkpoint-{global_step}\"\n",
        "                        )\n",
        "                        accelerator.save_state(save_path)\n",
        "                        logger.info(f\"Saved state to {save_path}\")\n",
        "\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "\n",
        "            if global_step >= max_train_steps:\n",
        "                break\n",
        "\n",
        "        # Validate!\n",
        "        if accelerator.is_main_process:\n",
        "            if epoch % args.validation_epochs == 0:\n",
        "                train_metrics = log_validation(\n",
        "                    store,\n",
        "                    real_train_dataloader,\n",
        "                    accelerator,\n",
        "                    epoch,\n",
        "                    split=\"train\",\n",
        "                    global_step=global_step,\n",
        "                    is_final_validation=False,\n",
        "                )\n",
        "                dev_metrics = log_validation(\n",
        "                    store,\n",
        "                    dev_dataloader,\n",
        "                    accelerator,\n",
        "                    epoch,\n",
        "                    split=\"dev\",\n",
        "                    global_step=global_step,\n",
        "                    is_final_validation=False,\n",
        "                )\n",
        "                dev_f1 = dev_metrics[\"f1\"]\n",
        "                if dev_f1 > best_dev_f1:\n",
        "                    best_dev_f1 = dev_f1\n",
        "                    best_dev_metrics = dev_metrics\n",
        "                    best_train_metrics = train_metrics\n",
        "\n",
        "    # Evaluate!\n",
        "    accelerator.wait_for_everyone()\n",
        "    if accelerator.is_main_process:\n",
        "        train_metrics = log_validation(\n",
        "            store,\n",
        "            real_train_dataloader,\n",
        "            accelerator,\n",
        "            epoch,\n",
        "            split=\"train\",\n",
        "            global_step=global_step,\n",
        "            is_final_validation=True,\n",
        "        )\n",
        "        dev_metrics = log_validation(\n",
        "            store,\n",
        "            dev_dataloader,\n",
        "            accelerator,\n",
        "            epoch,\n",
        "            split=\"dev\",\n",
        "            global_step=global_step,\n",
        "            is_final_validation=True,\n",
        "        )\n",
        "        dev_f1 = dev_metrics[\"f1\"]\n",
        "        if dev_f1 > best_dev_f1:\n",
        "            best_dev_f1 = dev_f1\n",
        "            best_dev_metrics = dev_metrics\n",
        "            best_train_metrics = train_metrics\n",
        "\n",
        "    # Finish\n",
        "    accelerator.end_training()\n",
        "    best_metrics = {\n",
        "        \"train\": best_train_metrics,\n",
        "        \"dev\": best_dev_metrics,\n",
        "    }\n",
        "    with open(os.path.join(args.output_dir, \"metrics.json\"), \"w\") as f:\n",
        "        json.dump(best_metrics, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "output": {
          "id": 1354444462747609,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Querying for '[<__main__.Document object at 0x7f3156284e00>, <__main__.Document object at 0x7f315626a1e0>, <__main__.Document object at 0x7f3156268230>, <__main__.Document object at 0x7f3156268830>]'\n",
            "1-1% of colorectal cancer patients are diagnosed with regional or distant metastases.Give me potential keywords of related articles.\n",
            "1 in 5 million in UK have abnormal PrP positivity.Give me potential keywords of related articles.\n",
            "10% of sudden infant death syndrome (SIDS) deaths happen in newborns aged less than 6 months.Give me potential keywords of related articles.\n",
            "0-dimensional biomaterials lack inductive properties.Give me potential keywords of related articles.\n",
            "Decoded tokens: [' RNA, the', ' myelod', 'ethylome of', ' of human newborn']\n",
            "Token IDs: [[[41214, 11, 279]], [[856, 301, 347]], [[42972, 638, 315]], [[315, 3823, 46397]]]\n",
            "Tokens: [41214, 11, 279]\n",
            "Found results:  [{'depth': 3, 'doc_ids': {2}, 'index_ids': [41214, 11, 279], 'index_txt': [' RNA', ',', ' the']}]\n",
            "Tokens: [856, 301, 347]\n",
            "Found results:  [{'depth': 3, 'doc_ids': {1}, 'index_ids': [856, 301, 347], 'index_txt': [' my', 'el', 'od']}]\n",
            "Tokens: [42972, 638, 315]\n",
            "Found results:  [{'depth': 3, 'doc_ids': {3}, 'index_ids': [42972, 638, 315], 'index_txt': ['ethyl', 'ome', ' of']}]\n",
            "Tokens: [315, 3823, 46397]\n",
            "Found results:  [{'depth': 3, 'doc_ids': {0}, 'index_ids': [315, 3823, 46397], 'index_txt': [' of', ' human', ' newborn']}]\n",
            "{'epoch': 19, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n"
          ]
        }
      ],
      "source": [
        "store.set_verbose_for_all(True)\n",
        "train_metrics = log_validation(\n",
        "    store,\n",
        "    real_train_dataloader,\n",
        "    accelerator,\n",
        "    epoch,\n",
        "    split=\"train\",\n",
        "    global_step=global_step,\n",
        "    is_final_validation=True,\n",
        ")\n",
        "print(train_metrics)\n",
        "store.set_verbose_for_all(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "output": {
          "id": 662559743105498,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(store._beams_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "163bf236-5e43-41d4-bfe9-d3fb65e76208",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "fbs (conda)",
      "language": "python",
      "name": "conda_fbs"
    },
    "language_info": {
      "name": "plaintext"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
